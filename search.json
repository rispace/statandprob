[
  {
    "objectID": "posts/bayesianinference/index.html",
    "href": "posts/bayesianinference/index.html",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "",
    "text": "Bayesian inference is a powerful statistical method that applies the principles of Bayes’s theorem to update the probability of a hypothesis as more evidence or information becomes available. It is widely used in various fields including machine learning, to make predictions and decisions under uncertainty.\n\nBayes’s theorem is a fundamental result in probability theory that relates the conditional and marginal probabilities of random events. Mathematically,\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\\hspace{4mm} \\implies \\mathbb{P}(A|B) \\propto \\mathbb{P}(B|A)\\mathbb{P}(A)\\]\nwhere, \\(A\\) and \\(B\\) are events and \\(\\mathbb{P}(B)\\ne 0\\).\n\n\\(\\mathbb{P}(A|B)\\) is a conditional probability which states the probability of occuring the event \\(A\\) when the event \\(B\\) is given or true. The other name of this quantity is called posterior probability of \\(A\\) given the event \\(B\\) or simply, posterior distribution.\n\n\\(\\mathbb{P}(B|A)\\) is a conditional probability which states the probability of occuring the event \\(B\\) when the event \\(A\\) is given or true. In other terms, \\(\\mathbb{P}(B|A)\\) is the likelihood: the probability of evidence \\(B\\) given that \\(A\\) is true.\n\n\\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B)\\) are the probabilities of occuring \\(A\\) and \\(B\\) respectively, without any dependence on each other. \\(\\mathbb{P}(A)\\) is called the prior probability or prior distribution and \\(\\mathbb{P}(B)\\) is called the marginal likelihood or marginal probabilities.\n\nExample 1\nConsider a medical example where we want to diagnose a disease based on a test result. Let:\n\n\\(D\\) be the event that a patient has the disease.\n\n\\(T\\) be the event that the test result is positive.\n\nWe are interested in finding the probability that a patient has the disease given a positive test result, \\(\\mathbb{P}(D|T)\\).\nGiven:\n\n\\(\\mathbb{P}(T|D) = 0.99\\) (the probability of a positive test result given the patient has the disease).\n\n\\(\\mathbb{P}(D) = 0.01\\) (the prior probability of the disease).\n\n\\(\\mathbb{P}(T|D') = 0.05\\) (the probability of a positive test result given the patient does not have the disease).\n\nFirst, we need to calculate the marginal likelihood \\(P(T)\\): \\[\\begin{align*}\n    \\mathbb{P}(T) &= \\mathbb{P}(T|D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(T|D') \\cdot \\mathbb{P}(D') \\\\\n    \\mathbb{P}(T) &= 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99\\\\\n    \\mathbb{P}(T) &= 0.0099 + 0.0495 \\\\\n    \\mathbb{P}(T) &= 0.0594\n\\end{align*}\\]\nNow, we can apply Bayes’s theorem:\n\\[\\begin{align*}\n    \\mathbb{P}(D|T) &= \\frac{\\mathbb{P}(T|D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(T)}\\\\\n    \\mathbb{P}(D|T) &= \\frac{0.99 \\cdot 0.01}{0.0594}\\\\\n    \\mathbb{P}(D|T) &\\approx 0.1667\n\\end{align*}\\]\nSo, the probability that the patient has the disease given a positive test result is approximately \\(16.67\\%\\).\nExample 2\n\n Assume that you are in a restuarant and you ordered a plate of 3 pancakes. The chef made three pancakes with one in perfect condition, that is not burnt in any side, one with one side burnt, and the last one burnt in both sides. The waiter wanted to stack the pancakes so that the burnt side does not show up when served. However, the chef recommended not to hide the burnt side and asked her to stack the pancakes randomly. What is the likelyhood that the fully burnt pancake will be on the top?  To solve this problem, we can use Bayesian approach. We denote the event \\(X\\) as the pancake without any burnt, \\(Y\\) with one side burnt, and \\(Z\\) both side burnt. Then we have the following conditional probabilities\n\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt} | X)&=0\\\\\n    \\mathbb{P}(\\text{top-burnt} | Y)&=\\frac{1}{2}\\\\\n    \\mathbb{P}(\\text{top-burnt} | Z)&=1\\\\\n\\end{align*}\\]\nThe probability of picking a pancake irrespective of their burnt condition is \\(\\frac{1}{3}\\). So,\n\\[\\begin{equation}\n    \\mathbb{P}(X)=\\mathbb{P}(Y)=\\mathbb{P}(Z)=\\frac{1}{3}\n\\end{equation}\\]\nThe marginal probability of having burnt side in the top position\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt})&=\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)\\\\\n    &=0\\cdot \\frac{1}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}+1\\cdot\\frac{1}{3}\\\\\n    &=\\frac{1}{2}\n\\end{align*}\\]\nNow, we can only have a burnt side on top if either \\(Z\\) is placed in the top or the burnt side of \\(Y\\) is placed in the top. \\[\\begin{align*}\n    \\mathbb{P}(Y|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{2}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3}\\\\\n    \\mathbb{P}(Z|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}\n\\end{align*}\\]\nSo the probability of having the fully burnt pancake on the top is \\(\\frac{2}{3}\\)."
  },
  {
    "objectID": "posts/bayesianinference/index.html#introduction",
    "href": "posts/bayesianinference/index.html#introduction",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "",
    "text": "Bayesian inference is a powerful statistical method that applies the principles of Bayes’s theorem to update the probability of a hypothesis as more evidence or information becomes available. It is widely used in various fields including machine learning, to make predictions and decisions under uncertainty.\n\nBayes’s theorem is a fundamental result in probability theory that relates the conditional and marginal probabilities of random events. Mathematically,\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}\\hspace{4mm} \\implies \\mathbb{P}(A|B) \\propto \\mathbb{P}(B|A)\\mathbb{P}(A)\\]\nwhere, \\(A\\) and \\(B\\) are events and \\(\\mathbb{P}(B)\\ne 0\\).\n\n\\(\\mathbb{P}(A|B)\\) is a conditional probability which states the probability of occuring the event \\(A\\) when the event \\(B\\) is given or true. The other name of this quantity is called posterior probability of \\(A\\) given the event \\(B\\) or simply, posterior distribution.\n\n\\(\\mathbb{P}(B|A)\\) is a conditional probability which states the probability of occuring the event \\(B\\) when the event \\(A\\) is given or true. In other terms, \\(\\mathbb{P}(B|A)\\) is the likelihood: the probability of evidence \\(B\\) given that \\(A\\) is true.\n\n\\(\\mathbb{P}(A)\\) or \\(\\mathbb{P}(B)\\) are the probabilities of occuring \\(A\\) and \\(B\\) respectively, without any dependence on each other. \\(\\mathbb{P}(A)\\) is called the prior probability or prior distribution and \\(\\mathbb{P}(B)\\) is called the marginal likelihood or marginal probabilities.\n\nExample 1\nConsider a medical example where we want to diagnose a disease based on a test result. Let:\n\n\\(D\\) be the event that a patient has the disease.\n\n\\(T\\) be the event that the test result is positive.\n\nWe are interested in finding the probability that a patient has the disease given a positive test result, \\(\\mathbb{P}(D|T)\\).\nGiven:\n\n\\(\\mathbb{P}(T|D) = 0.99\\) (the probability of a positive test result given the patient has the disease).\n\n\\(\\mathbb{P}(D) = 0.01\\) (the prior probability of the disease).\n\n\\(\\mathbb{P}(T|D') = 0.05\\) (the probability of a positive test result given the patient does not have the disease).\n\nFirst, we need to calculate the marginal likelihood \\(P(T)\\): \\[\\begin{align*}\n    \\mathbb{P}(T) &= \\mathbb{P}(T|D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(T|D') \\cdot \\mathbb{P}(D') \\\\\n    \\mathbb{P}(T) &= 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99\\\\\n    \\mathbb{P}(T) &= 0.0099 + 0.0495 \\\\\n    \\mathbb{P}(T) &= 0.0594\n\\end{align*}\\]\nNow, we can apply Bayes’s theorem:\n\\[\\begin{align*}\n    \\mathbb{P}(D|T) &= \\frac{\\mathbb{P}(T|D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(T)}\\\\\n    \\mathbb{P}(D|T) &= \\frac{0.99 \\cdot 0.01}{0.0594}\\\\\n    \\mathbb{P}(D|T) &\\approx 0.1667\n\\end{align*}\\]\nSo, the probability that the patient has the disease given a positive test result is approximately \\(16.67\\%\\).\nExample 2\n\n Assume that you are in a restuarant and you ordered a plate of 3 pancakes. The chef made three pancakes with one in perfect condition, that is not burnt in any side, one with one side burnt, and the last one burnt in both sides. The waiter wanted to stack the pancakes so that the burnt side does not show up when served. However, the chef recommended not to hide the burnt side and asked her to stack the pancakes randomly. What is the likelyhood that the fully burnt pancake will be on the top?  To solve this problem, we can use Bayesian approach. We denote the event \\(X\\) as the pancake without any burnt, \\(Y\\) with one side burnt, and \\(Z\\) both side burnt. Then we have the following conditional probabilities\n\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt} | X)&=0\\\\\n    \\mathbb{P}(\\text{top-burnt} | Y)&=\\frac{1}{2}\\\\\n    \\mathbb{P}(\\text{top-burnt} | Z)&=1\\\\\n\\end{align*}\\]\nThe probability of picking a pancake irrespective of their burnt condition is \\(\\frac{1}{3}\\). So,\n\\[\\begin{equation}\n    \\mathbb{P}(X)=\\mathbb{P}(Y)=\\mathbb{P}(Z)=\\frac{1}{3}\n\\end{equation}\\]\nThe marginal probability of having burnt side in the top position\n\\[\\begin{align*}\n    \\mathbb{P}(\\text{top-burnt})&=\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)\\\\\n    &=0\\cdot \\frac{1}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}+1\\cdot\\frac{1}{3}\\\\\n    &=\\frac{1}{2}\n\\end{align*}\\]\nNow, we can only have a burnt side on top if either \\(Z\\) is placed in the top or the burnt side of \\(Y\\) is placed in the top. \\[\\begin{align*}\n    \\mathbb{P}(Y|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{2}\\cdot\\frac{1}{3}}{\\frac{1}{2}}=\\frac{1}{3}\\\\\n    \\mathbb{P}(Z|\\text{top-burnt})&=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt})}\\\\\n    &=\\frac{\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}{\\mathbb{P}(\\text{top-burnt}|X)\\mathbb{P}(X)+\\mathbb{P}(\\text{top-burnt}|Y)\\mathbb{P}(Y)+\\mathbb{P}(\\text{top-burnt}|Z)\\mathbb{P}(Z)}\\\\\n    &=\\frac{\\frac{1}{3}}{\\frac{1}{2}}=\\frac{2}{3}\n\\end{align*}\\]\nSo the probability of having the fully burnt pancake on the top is \\(\\frac{2}{3}\\)."
  },
  {
    "objectID": "posts/bayesianinference/index.html#why-bayesian-inference-in-machine-learning",
    "href": "posts/bayesianinference/index.html#why-bayesian-inference-in-machine-learning",
    "title": "Bayesian Inference in Machine Learning: Part 1",
    "section": "Why Bayesian Inference in Machine Learning?",
    "text": "Why Bayesian Inference in Machine Learning?\nBayesian inference plays a crucial role in machine learning, particularly in areas involving uncertainty and probabilistic reasoning. It allows us to incorporate prior knowledge and update beliefs based on new data, which is especially useful in the following applications:\n\nBayesian Networks\nBayesian networks are graphical models that represent the probabilistic relationships among a set of variables. Each node in the network represents a random variable, and the edges represent conditional dependencies. Bayesian networks are used for various tasks such as classification, prediction, and anomaly detection.\n\n\nBayesian Regression\nBayesian regression extends linear regression by incorporating prior distributions on the model parameters. This approach provides a probabilistic framework for regression analysis, allowing for uncertainty in the parameter estimates. The posterior distribution of the parameters is computed using Bayes’s theorem, and predictions are made by averaging over this distribution.\n\n\nSampling Methods\nIn Bayesian inference, exact computation of the posterior distribution is often intractable. Therefore, sampling methods such as Markov Chain Monte Carlo (MCMC) and Variational Inference are used to approximate the posterior distribution. These methods generate samples from the posterior distribution, allowing us to estimate various statistical properties and make inferences.\nMarkov Chain Monte Carlo (MCMC)\nMCMC methods generate a sequence of samples from the posterior distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution. Common MCMC algorithms include the Underdamped and Overdamped Langevin dynamics, Metropolis-Hastings algorithm and the Gibbs sampler.\nExample: Metropolis-Hastings Algorithm\nConsider a posterior distribution \\(P(\\theta|D)\\) where \\(\\theta\\) represents the model parameters and \\(D\\) represents the data. The Metropolis-Hastings algorithm proceeds as follows:\n\nInitialize the parameters \\(\\theta_0\\).\nFor \\(t = 1\\) to \\(T\\):\n\nPropose a new state \\(\\theta'\\) from a proposal distribution \\(Q(\\theta'|\\theta_t)\\).\nCompute the acceptance ratio \\(\\alpha = \\frac{P(\\theta'|D) \\cdot Q(\\theta_t|\\theta')}{P(\\theta_t|D) \\cdot Q(\\theta'|\\theta_t)}\\).\nAccept the new state with probability \\(\\min(1, \\alpha)\\). If accepted, set \\(\\theta_{t+1} = \\theta'\\); otherwise, set \\(\\theta_{t+1} = \\theta_t\\).\n\n\nThe samples \\(\\theta_1, \\theta_2, \\ldots, \\theta_T\\) form a Markov chain whose stationary distribution is the posterior distribution \\(P(\\theta|D)\\).\n\n\nBayesian Inference in Neural Networks\nBayesian methods are also applied to neural networks, resulting in Bayesian Neural Networks (BNNs). BNNs incorporate uncertainty in the network weights by placing a prior distribution over them and using Bayes’s theorem to update this distribution based on the observed data. This allows BNNs to provide not only point estimates but also uncertainty estimates for their predictions.\nIn the next parts, we will talk about different applications of the Bayesian inferences, specifically, sampling problem using Langevin dynamics.\n\n\nReference\n\nPancake problems on mathstackexchance\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/optionprice/index.html",
    "href": "posts/optionprice/index.html",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "",
    "text": "In this blog, we will explore how to price simple equity derivatives using the Black-Scholes-Merton (BSM) model. We will derive the mathematical formula and then provide Python code to implement it.\n\n\nBefore proceeding to the deep of the discussion, we need to know some definition and terminology\n\nBrownian Motion: Brownian motion is a concept with definitions and applications across various disciplines, named after the botanist Robert Brown, is the random, erratic movement of particles suspended in a fluid (liquid or gas) due to their collisions with the fast-moving molecules of the fluid.\n\nBrownian motion is a stochastic process \\((B_t)_{t \\geq 0}\\) defined as a continuous-time process with the following properties:\n\n\\(B_0 = 0\\) almost surely.\n\\(B_t\\) has independent increments.\nFor \\(t &gt; s\\), \\(B_t - B_s \\sim N(0, t-s)\\) (normally distributed with mean 0 and variance \\(t-s\\)).\n\\(B_t\\) has continuous paths almost surely.\n\n\n\nCode\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nn_steps = 100  # Number of steps\nn_paths = 20   # Number of paths\ntime_horizon = 1  # Total time\ndt = time_horizon / n_steps  # Time step\nt = np.linspace(0, time_horizon, n_steps)  # Time array\n\n# Generate Brownian motion\ndef generate_brownian_paths(n_paths, n_steps, dt):\n    # Standard normal increments scaled by sqrt(dt)\n    increments = np.random.normal(0, np.sqrt(dt), (n_paths, n_steps))\n    # Cumulative sum to generate paths\n    return np.cumsum(increments, axis=1)\n\n# Generate one path and multiple paths\nsingle_path = generate_brownian_paths(1, n_steps, dt)[0]\nmultiple_paths = generate_brownian_paths(n_paths, n_steps, dt)\n\n# Plotting\nfig, axes = plt.subplots(1, 2, figsize=(7.9, 3.9))\n\n# Single path\naxes[0].plot(t, single_path, label=\"Single Path\")\naxes[0].set_title(\"Brownian Motion: Single Path\")\naxes[0].set_xlabel(\"Time\")\naxes[0].set_ylabel(\"Position\")\naxes[0].legend()\n\n# Multiple paths\nfor path in multiple_paths:\n    axes[1].plot(t, path, alpha=0.5, linewidth=0.8)\naxes[1].set_title(f\"Brownian Motion: {n_paths} Paths\")\naxes[1].set_xlabel(\"Time\")\naxes[1].set_ylabel(\"Position\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nGeometric Brownian Motion (GBM)\nA stochastic process \\(S_t\\) is said to follow a geometric Brownian motion if it satisfies the following equation:\n\\[\ndS_t = \\mu S_t dt+\\sigma S_t dB_t\n\\]\nWhich can be written as\n\\[\nS_t - S_0 =\\int_0^t \\mu S_u du + \\int_0^t \\sigma S_u dB_u\n\\]\nTo solve the GBM, we apply Ito’s formula to the function \\(Z_t = f(t, S_t)= \\ln(S_t)\\) and then by Taylor’s expansion, we have\n\\[\\begin{align*}\ndf & = \\frac{\\partial f}{\\partial t}dt+ \\frac{\\partial f}{\\partial s}dS_t + \\frac{1}{2} \\frac{\\partial ^2f}{\\partial s^2}(dS_t)^2+\\frac{1}{2}\\frac{\\partial ^2f}{\\partial s^2}(dt)^2+\\frac{\\partial^2 f}{\\partial t\\partial s}dtdS_t\n\\end{align*}\\]\nBy definition we have \\[\\begin{align*}\ndS_t &= \\mu S_t dt+\\sigma S_t dB_t\\\\\n(dS_t)^2 & = \\mu^2 (dt)^2+2\\mu \\sigma dt dB_t + \\sigma^2 (dB_t)^2\n\\end{align*}\\]\nThe term \\((dt)^2\\) is negligible compared to the term \\(dt\\) and it is also assume that the product \\(dtdB_t\\) is negligible. Furthermore, the quadratic variation of \\(B_t\\) i.e., \\((dB_t)^2= dt\\). With these values, we obtain\n\\[\\begin{align*}\ndZ_t & = \\frac{1}{S_t} dS_t + \\frac{1}{2} \\left\\{-\\frac{1}{S_t^2}\\right\\}[dS_t]^2\\\\\n& =  \\frac{1}{S_t} (\\mu S_t dt+\\sigma S_t dB_t) + \\frac{1}{2} \\left\\{-\\frac{1}{S_t^2}\\right\\}\\sigma^2S_t^2dt\\\\\n\\implies dZ_t &= (\\mu dt +\\sigma dB_t) -\\frac{1}{2}\\sigma^2 dt\\\\\n& = \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)dt+\\sigma dB_t\n\\end{align*}\\]\nwith \\(Z_0=\\ln S_0\\). Now we have the following\n\\[\\begin{align*}\n\\int_0^t dZ_s &= \\int_0^t \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)ds + \\int_0^t\\sigma dB_s\\\\\n\\implies Z_t - Z_0 &= \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\\\\n\\implies \\ln S_t - \\ln S_0&= \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\\\\n\\implies \\ln \\left(\\frac{S_t}{S_0}\\right) &= \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\\\\n\\implies S_t &= S_0 \\exp{\\left\\{\\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\right\\}}\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/optionprice/index.html#introduction",
    "href": "posts/optionprice/index.html#introduction",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "",
    "text": "In this blog, we will explore how to price simple equity derivatives using the Black-Scholes-Merton (BSM) model. We will derive the mathematical formula and then provide Python code to implement it.\n\n\nBefore proceeding to the deep of the discussion, we need to know some definition and terminology\n\nBrownian Motion: Brownian motion is a concept with definitions and applications across various disciplines, named after the botanist Robert Brown, is the random, erratic movement of particles suspended in a fluid (liquid or gas) due to their collisions with the fast-moving molecules of the fluid.\n\nBrownian motion is a stochastic process \\((B_t)_{t \\geq 0}\\) defined as a continuous-time process with the following properties:\n\n\\(B_0 = 0\\) almost surely.\n\\(B_t\\) has independent increments.\nFor \\(t &gt; s\\), \\(B_t - B_s \\sim N(0, t-s)\\) (normally distributed with mean 0 and variance \\(t-s\\)).\n\\(B_t\\) has continuous paths almost surely.\n\n\n\nCode\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nn_steps = 100  # Number of steps\nn_paths = 20   # Number of paths\ntime_horizon = 1  # Total time\ndt = time_horizon / n_steps  # Time step\nt = np.linspace(0, time_horizon, n_steps)  # Time array\n\n# Generate Brownian motion\ndef generate_brownian_paths(n_paths, n_steps, dt):\n    # Standard normal increments scaled by sqrt(dt)\n    increments = np.random.normal(0, np.sqrt(dt), (n_paths, n_steps))\n    # Cumulative sum to generate paths\n    return np.cumsum(increments, axis=1)\n\n# Generate one path and multiple paths\nsingle_path = generate_brownian_paths(1, n_steps, dt)[0]\nmultiple_paths = generate_brownian_paths(n_paths, n_steps, dt)\n\n# Plotting\nfig, axes = plt.subplots(1, 2, figsize=(7.9, 3.9))\n\n# Single path\naxes[0].plot(t, single_path, label=\"Single Path\")\naxes[0].set_title(\"Brownian Motion: Single Path\")\naxes[0].set_xlabel(\"Time\")\naxes[0].set_ylabel(\"Position\")\naxes[0].legend()\n\n# Multiple paths\nfor path in multiple_paths:\n    axes[1].plot(t, path, alpha=0.5, linewidth=0.8)\naxes[1].set_title(f\"Brownian Motion: {n_paths} Paths\")\naxes[1].set_xlabel(\"Time\")\naxes[1].set_ylabel(\"Position\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nGeometric Brownian Motion (GBM)\nA stochastic process \\(S_t\\) is said to follow a geometric Brownian motion if it satisfies the following equation:\n\\[\ndS_t = \\mu S_t dt+\\sigma S_t dB_t\n\\]\nWhich can be written as\n\\[\nS_t - S_0 =\\int_0^t \\mu S_u du + \\int_0^t \\sigma S_u dB_u\n\\]\nTo solve the GBM, we apply Ito’s formula to the function \\(Z_t = f(t, S_t)= \\ln(S_t)\\) and then by Taylor’s expansion, we have\n\\[\\begin{align*}\ndf & = \\frac{\\partial f}{\\partial t}dt+ \\frac{\\partial f}{\\partial s}dS_t + \\frac{1}{2} \\frac{\\partial ^2f}{\\partial s^2}(dS_t)^2+\\frac{1}{2}\\frac{\\partial ^2f}{\\partial s^2}(dt)^2+\\frac{\\partial^2 f}{\\partial t\\partial s}dtdS_t\n\\end{align*}\\]\nBy definition we have \\[\\begin{align*}\ndS_t &= \\mu S_t dt+\\sigma S_t dB_t\\\\\n(dS_t)^2 & = \\mu^2 (dt)^2+2\\mu \\sigma dt dB_t + \\sigma^2 (dB_t)^2\n\\end{align*}\\]\nThe term \\((dt)^2\\) is negligible compared to the term \\(dt\\) and it is also assume that the product \\(dtdB_t\\) is negligible. Furthermore, the quadratic variation of \\(B_t\\) i.e., \\((dB_t)^2= dt\\). With these values, we obtain\n\\[\\begin{align*}\ndZ_t & = \\frac{1}{S_t} dS_t + \\frac{1}{2} \\left\\{-\\frac{1}{S_t^2}\\right\\}[dS_t]^2\\\\\n& =  \\frac{1}{S_t} (\\mu S_t dt+\\sigma S_t dB_t) + \\frac{1}{2} \\left\\{-\\frac{1}{S_t^2}\\right\\}\\sigma^2S_t^2dt\\\\\n\\implies dZ_t &= (\\mu dt +\\sigma dB_t) -\\frac{1}{2}\\sigma^2 dt\\\\\n& = \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)dt+\\sigma dB_t\n\\end{align*}\\]\nwith \\(Z_0=\\ln S_0\\). Now we have the following\n\\[\\begin{align*}\n\\int_0^t dZ_s &= \\int_0^t \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)ds + \\int_0^t\\sigma dB_s\\\\\n\\implies Z_t - Z_0 &= \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\\\\n\\implies \\ln S_t - \\ln S_0&= \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\\\\n\\implies \\ln \\left(\\frac{S_t}{S_0}\\right) &= \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\\\\n\\implies S_t &= S_0 \\exp{\\left\\{\\left(\\mu-\\frac{1}{2}\\sigma^2\\right)t + \\sigma B_t\\right\\}}\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/optionprice/index.html#black-scholes-merton-formula",
    "href": "posts/optionprice/index.html#black-scholes-merton-formula",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "Black-Scholes-Merton Formula",
    "text": "Black-Scholes-Merton Formula\nNow we are ready to derive the BSM PDE. The payoff of an option \\(V(S,T)\\) at maturity is is known. To find the value at an earlier stage, we need to know how V behaves as a function of \\(S\\) and \\(t\\). By Ito’s lemma we have \\[\\begin{align*}\ndV& = \\left(\\mu S\\frac{\\partial V}{\\partial S}+\\frac{\\partial V}{\\partial t}+\\frac{1}{2}\\sigma^2S^2\\frac{\\partial^2 V}{\\partial S^2}\\right)dt+\\sigma S\\frac{\\partial V}{\\partial S}dB.\n\\end{align*}\\]\nNow let’s consider a portfolio consisting of a short one option and long \\(\\frac{\\partial V}{\\partial S}\\) shares at time \\(t\\). The value of this portfolio is\n\\[\n\\Pi = -V+\\frac{\\partial V}{\\partial S}S\n\\]\nover the time \\([t,t+\\Delta t]\\), the total profit or loss from the changes in the values of the portfolio is \\[\n\\Delta \\Pi = -\\Delta V + \\frac{\\partial V}{\\partial S}\\Delta S\n\\]\nNow by the discretization we have, \\[\\begin{align*}\n\\Delta S &= \\mu S \\Delta t +\\sigma S \\Delta B\\\\\n\\Delta V & = \\left(\\mu S\\frac{\\partial V}{\\partial S}+\\frac{\\partial V}{\\partial t}+\\frac{1}{2}\\sigma^2S^2\\frac{\\partial^2 V}{\\partial S^2}\\right)\\Delta t+\\sigma S\\frac{\\partial V}{\\partial S}\\Delta B\\\\\n\\implies \\Delta \\Pi  & = \\left(-\\frac{\\partial V}{\\partial t} -\\frac{1}{2}\\sigma^2S^2\\frac{\\partial^2 V}{\\partial S^2}\\right)\\Delta t\n\\end{align*}\\]\nAt this point, if \\(r\\) is the risk-free interest rate then we will have following relationship \\[\nr\\Pi \\Delta t = \\Delta \\Pi\n\\]\nThe rationale of this relation is that no-aribtrage assumption. Thus, we have\n\\[\\begin{align*}\n\\left(-\\frac{\\partial V}{\\partial t} -\\frac{1}{2}\\sigma^2S^2\\frac{\\partial^2 V}{\\partial S^2}\\right)\\Delta t & = r \\left(- V + \\frac{\\partial V}{\\partial S} S\\right)\\Delta t \\\\\n\\implies \\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} + rS \\frac{\\partial V}{\\partial S} -rV &=0\n\\end{align*}\\]\nThis is the famous Black-Scholes-Merton PDF, formally written with the boundary conditions as follows\n\\[\\begin{align*}\n\\frac{\\partial c}{\\partial t} + \\frac{1}{2} \\sigma^2 c^2 \\frac{\\partial^2 c}{\\partial S^2} + rc \\frac{\\partial c}{\\partial S} -rc &=0\\\\\nc(0,t) &= 0\\\\\nc(S_{+\\infty}, t) &= S - Ke^{-r(T-t)}\\\\\nc(S,T) & = max\\{S-K,0\\}\n\\end{align*}\\]\nThis Black-Scholes-Merton PDE can be reduced to the heat equation using the substitutions \\(S = K e^x\\), \\(t = T - \\frac{\\tau}{\\frac{1}{2} \\sigma^2}\\), and \\(c(S, t) = K v(x, \\tau)\\). Let’s derive the solution step by step in full mathematical detail and show how this leads to the normal CDF.\n\nStep 1: Substitutions\nWe aim to reduce the BSM PDE: \\[\n\\frac{\\partial c}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 c}{\\partial S^2} + r S \\frac{\\partial c}{\\partial S} - r c = 0\n\\]\nto the heat equation. Using the substitutions:\n\n\\(S = K e^x\\), where \\(x = \\ln(S / K)\\), and \\(S \\in (0, \\infty)\\) maps \\(x \\in (-\\infty, \\infty)\\),\n\\(t = T - \\frac{\\tau}{\\frac{1}{2} \\sigma^2}\\), so \\(\\tau = \\frac{1}{2} \\sigma^2 (T - t)\\),\n\\(c(S, t) = K v(x, \\tau)\\), where \\(v(x, \\tau)\\) is the transformed function.\n\n\n\nStep 2: Derivative Transformations\nFor \\(c(S, t) = K v(x, \\tau)\\), we compute derivatives.\n\nThe first derivative of \\(c\\) with respect to \\(S\\): \\[\n\\frac{\\partial c}{\\partial S} = \\frac{\\partial}{\\partial S} \\big(K v(x, \\tau)\\big) = K \\frac{\\partial v}{\\partial x} \\frac{\\partial x}{\\partial S},\n\\] where \\(x = \\ln(S / K)\\) implies \\(\\frac{\\partial x}{\\partial S} = \\frac{1}{S}\\). Thus: \\[\n\\frac{\\partial c}{\\partial S} = K \\frac{\\partial v}{\\partial x} \\frac{1}{S}.\n\\]\nThe second derivative of \\(c\\) with respect to \\(S\\): \\[\n\\frac{\\partial^2 c}{\\partial S^2} = \\frac{\\partial}{\\partial S} \\left( K \\frac{\\partial v}{\\partial x} \\frac{1}{S} \\right).\n\\] Using the product rule: \\[\n\\frac{\\partial^2 c}{\\partial S^2} = K \\frac{\\partial^2 v}{\\partial x^2} \\frac{1}{S^2} - K \\frac{\\partial v}{\\partial x} \\frac{1}{S^2}.\n\\]\nThe time derivative: \\[\n\\frac{\\partial c}{\\partial t} = K \\frac{\\partial v}{\\partial \\tau} \\frac{\\partial \\tau}{\\partial t}, \\quad \\text{and } \\frac{\\partial \\tau}{\\partial t} = -\\frac{1}{\\frac{1}{2} \\sigma^2}.\n\\]\n\n\n\nStep 3: Transforming the PDE\nSubstituting the above derivatives into the BSM PDE, we rewrite each term.\n\nFor \\(\\frac{\\partial c}{\\partial t}\\): \\[\n\\frac{\\partial c}{\\partial t} = -\\frac{1}{\\frac{1}{2} \\sigma^2} K \\frac{\\partial v}{\\partial \\tau}.\n\\]\nFor \\(\\frac{\\partial c}{\\partial S}\\): \\[\nS \\frac{\\partial c}{\\partial S} = S \\cdot \\left(K \\frac{\\partial v}{\\partial x} \\frac{1}{S}\\right) = K \\frac{\\partial v}{\\partial x}.\n\\]\nFor \\(\\frac{\\partial^2 c}{\\partial S^2}\\): \\[\n\\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 c}{\\partial S^2} = \\frac{1}{2} \\sigma^2 S^2 \\left(K \\frac{\\partial^2 v}{\\partial x^2} \\frac{1}{S^2} - K \\frac{\\partial v}{\\partial x} \\frac{1}{S^2}\\right) = \\frac{1}{2} \\sigma^2 K \\frac{\\partial^2 v}{\\partial x^2}.\n\\]\n\nSubstituting all these into the BSM PDE: \\[\n-\\frac{1}{\\frac{1}{2} \\sigma^2} K \\frac{\\partial v}{\\partial \\tau} + \\frac{1}{2} \\sigma^2 K \\frac{\\partial^2 v}{\\partial x^2} + r K \\frac{\\partial v}{\\partial x} - r K v = 0.\n\\]\nDivide through by \\(K\\): \\[\n-\\frac{\\partial v}{\\partial \\tau} + \\frac{\\partial^2 v}{\\partial x^2} + \\frac{2r}{\\sigma^2} \\frac{\\partial v}{\\partial x} - \\frac{2r}{\\sigma^2} v = 0.\n\\]\nTo simplify, let \\(v(x, \\tau) = e^{\\alpha x + \\beta \\tau} u(x, \\tau)\\), where \\(\\alpha\\) and \\(\\beta\\) are constants. Substituting and choosing \\(\\alpha = -\\frac{r}{\\sigma^2}\\) and \\(\\beta = -\\frac{r^2}{2 \\sigma^2}\\), the equation reduces to: \\[\n\\frac{\\partial u}{\\partial \\tau} = \\frac{\\partial^2 u}{\\partial x^2}.\n\\]\n\n\nStep 4: Solving the Heat Equation\nThe heat equation \\(\\frac{\\partial u}{\\partial \\tau} = \\frac{\\partial^2 u}{\\partial x^2}\\) has a well-known solution using Fourier methods: \\[\nu(x, \\tau) = \\frac{1}{\\sqrt{2 \\pi \\tau}} \\int_{-\\infty}^\\infty e^{-\\frac{(x-y)^2}{2\\tau}} f(y) \\, dy,\n\\]\nwhere \\(f(y)\\) is the initial condition.\nFor the BSM problem, the initial condition is the payoff: \\[\nf(y) = \\max(e^y - 1, 0).\n\\]\nPerforming the integration leads to the final solution involving the cumulative normal distribution function: \\[\nv(x, \\tau) = N(d_1) - e^{-x} N(d_2),\n\\]\nwhere: \\[\nd_1 = \\frac{x + \\frac{1}{2} \\tau}{\\sqrt{\\tau}}, \\quad d_2 = \\frac{x - \\frac{1}{2} \\tau}{\\sqrt{\\tau}}.\n\\]\nTransforming back to the original variables gives the Black-Scholes formula: \\[\nC(S, t) = S e^{-q(T-t)} N(d_1) - K e^{-r(T-t)} N(d_2),\n\\] where: \\[\nd_1 = \\frac{\\ln(S / K) + (r - q + \\frac{\\sigma^2}{2})(T-t)}{\\sigma \\sqrt{T-t}}, \\quad d_2 = d_1 - \\sigma \\sqrt{T-t}.\n\\]\nSimilarly, we can derive the price of a European put option:\n\\[\nP = K e^{-rT} N(-d_2) - S e^{-qT} N(-d_1)\n\\]\nWhere: \\[\nd_1 = \\frac{\\ln(\\frac{S}{K}) + (r - q + \\frac{\\sigma^2}{2})T}{\\sigma \\sqrt{T}}, \\quad d_2 = d_1 - \\sigma \\sqrt{T}\n\\]\n\n\nAsymptotic Behavior of the BSM formula for call and put options\nWhat if \\(K\\rightarrow 0\\)? In that case,\n\n\\(\\ln(S_0/K)\\rightarrow \\infty\\), causing \\(d_1 \\rightarrow \\infty\\) and \\(d_2 \\rightarrow \\infty\\)\n\nThe cdf \\(N(d_1)\\rightarrow 1\\) and \\(N(d_2)\\rightarrow 1\\)\n\nThe second term \\(Ke^{-rT}N(d_2)\\rightarrow 0\\) as \\(K\\rightarrow 0\\)\n\nIn this case, the price of a call option \\(C\\rightarrow S_0\\) and the price of a put option \\(P \\rightarrow 0\\)"
  },
  {
    "objectID": "posts/optionprice/index.html#greeks-delta-and-gamma",
    "href": "posts/optionprice/index.html#greeks-delta-and-gamma",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "Greeks: Delta and Gamma",
    "text": "Greeks: Delta and Gamma\nDelta (\\(\\Delta\\)) is the sensitivity of the option price to changes in the underlying asset price:\n\\[\n\\Delta = \\frac{\\partial C}{\\partial S}\\approx \\frac{C(S_0 + h) - C(S_0 - h)}{2h}\n\\]\nThis is the central difference approximation, which provides a more accurate estimate of delta compared to the forward or backward difference methods.\n\n\\(C(S_0 + h)\\): Calculate the option price with the spot price increased by \\(h\\).\n\\(C(S_0 - h)\\): Calculate the option price with the spot price decreased by \\(h\\).\n\nGamma (\\(\\Gamma\\)) measures the rate of change of delta with respect to the underlying asset price:\n\\[\n\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}\\approx \\frac{\\Delta(S_0 + h) - \\Delta(S_0 - h)}{2h}\\approx \\frac{C(S_0 + h) - 2C(S_0) + C(S_0 - h)}{h^2}\n\\]\nGamma (\\(\\Gamma\\)) measures the rate of change of delta (\\(\\Delta\\)) with respect to the underlying spot price (\\(S_0\\)).\n\n\\(C(S_0 + h)\\): Option price with the spot price increased by \\(h\\).\n\\(C(S_0)\\): Option price at the current spot price.\n\\(C(S_0 - h)\\): Option price with the spot price decreased by \\(h\\).\n\nRelationship Between Delta and Gamma:\n\nGamma represents how much delta changes for a small change in \\(S_0\\).\nIf gamma is high, delta is more sensitive to changes in \\(S_0\\), which is important for hedging strategies."
  },
  {
    "objectID": "posts/optionprice/index.html#implementation",
    "href": "posts/optionprice/index.html#implementation",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "Implementation",
    "text": "Implementation\n\nNotation\n\n\\(S\\): Spot price of the stock.\n\\(K\\): Strike price of the option.\n\\(T\\): Time to maturity (in years).\n\\(r\\): Risk-free rate (continuously compounded).\n\\(q\\): Dividend yield (continuously compounded).\n\\(\\sigma\\): Volatility of the stock.\n\\(N(\\cdot)\\): Cumulative distribution function of the standard normal distribution.\n\n\nfrom dataclasses import dataclass\nimport numpy as np\nfrom scipy.stats import norm\n\n@dataclass\nclass Equity:\n    spot: float\n    dividend_yield: float\n    volatility: float\n\n@dataclass\nclass EquityOption:\n    strike: float\n    time_to_maturity: float\n    put_call: str\n\n@dataclass\nclass EquityForward:\n    strike: float\n    time_to_maturity: float\n\ndef bsm(underlying: Equity, option: EquityOption, rate: float) -&gt; float:\n    S = underlying.spot\n    K = option.strike\n    T = option.time_to_maturity\n    r = rate\n    q = underlying.dividend_yield\n    sigma = underlying.volatility\n\n    # Handle edge case where strike is effectively zero\n    if K &lt; 1e-8:\n        if option.put_call.lower() == \"call\":\n            return S \n        else:\n            return 0.0\n\n    d1 = (np.log(S / K) + (r - q + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n    d2 = d1 - sigma * np.sqrt(T)\n\n    if option.put_call.lower() == \"call\":\n        price = S * np.exp(-q * T) * norm.cdf(d1) \\\n                - K * np.exp(-r * T) * norm.cdf(d2)\n    elif option.put_call.lower() == \"put\":\n        price = K * np.exp(-r * T) * norm.cdf(-d2) \\\n                - S * np.exp(-q * T) * norm.cdf(-d1)\n    else:\n        raise ValueError(\"Invalid option type. Must be 'call' or 'put'.\")\n\n    return price\n\ndef delta(underlying: Equity, option: EquityOption, rate: float) -&gt; float:\n    bump = 0.01 * underlying.spot\n    bumped_up = Equity(spot=underlying.spot + bump, \n                       dividend_yield=underlying.dividend_yield, \n                       volatility=underlying.volatility)\n    bumped_down = Equity(spot=underlying.spot - bump, \n                         dividend_yield=underlying.dividend_yield, \n                         volatility=underlying.volatility)\n    price_up = bsm(bumped_up, option, rate)\n    price_down = bsm(bumped_down, option, rate)\n    return (price_up - price_down) / (2 * bump)\n\ndef gamma(underlying: Equity, option: EquityOption, rate: float) -&gt; float:\n    bump = 0.01 * underlying.spot\n    bumped_up = Equity(spot=underlying.spot + bump, \n                       dividend_yield=underlying.dividend_yield, \n                       volatility=underlying.volatility)\n    bumped_down = Equity(spot=underlying.spot - bump, \n                         dividend_yield=underlying.dividend_yield, \n                         volatility=underlying.volatility)\n    original_price = bsm(underlying, option, rate)\n    price_up = bsm(bumped_up, option, rate)\n    price_down = bsm(bumped_down, option, rate)\n    return (price_up - 2 * original_price + price_down) / (bump**2)\n\ndef fwd(underlying: Equity, forward: EquityForward, rate: float) -&gt; float:\n    S = underlying.spot\n    K = forward.strike\n    T = forward.time_to_maturity\n    r = rate\n    q = underlying.dividend_yield\n    forward_price = S * np.exp((r - q) * T) - K\n\n    return forward_price\n\ndef check_put_call_parity(\n    underlying: Equity, \n    call_option: EquityOption, \n    put_option: EquityOption, \n    rate: float\n    ) -&gt; bool:\n\n    call_price = bsm(underlying, call_option, rate)\n    put_price = bsm(underlying, put_option, rate)\n    S = underlying.spot\n    K = call_option.strike\n    T = call_option.time_to_maturity\n    r = rate\n    q = underlying.dividend_yield\n\n    parity_lhs = call_price - put_price\n    parity_rhs = S * np.exp(-q * T) - K * np.exp(-r * T)\n\n    return np.isclose(parity_lhs, parity_rhs, atol=1e-4)\n\n\n\nExample Usage\n\nSay, we want to price a call option on an equity with spot price \\(S_0 = 450\\) with dividend yield \\(q=1.4\\%\\), and volatility \\(14\\%\\). The strike price of the call is \\(K=470\\), with time to maturity in years \\(T=0.23\\) and the risk free rate \\(r = 0.05\\).  Next, we want to see the asymptotic behavior of the call option if the strike price \\(K\\rightarrow 0\\) with interest rate 0.  Next, we want to price a put option on the same equity but strike price \\(K=500\\), time to maturity in years \\(T=0.26\\) and interest rate is 0.  Finally, we want to check if the put-call parity relationship is hold.   In each case, we consider \\(h=0.01\\) a bump or small change in the stock price.\n\n\nif __name__ == \"__main__\":\n    eq = Equity(450, 0.014, 0.14)\n    option_call = EquityOption(470, 0.23, \"call\")\n    option_put = EquityOption(500, 0.26, \"put\")\n    \n    print(bsm(eq, option_call, 0.05))  \n    print(bsm(eq, EquityOption(1e-15, 0.26, \"call\"), 0.0))  \n    print(bsm(Equity(450, 0.0, 1e-9), option_put, 0.0))  \n\n    # Check put-call parity\n    eq = Equity(450, 0.015, 0.15)\n    option_call = EquityOption(470, 0.26, \"call\")\n    option_put = EquityOption(470, 0.26, \"put\")\n    print(check_put_call_parity(eq, option_call, option_put, 0.05)) \n\n5.834035584709966\n450\n50.0\nTrue"
  },
  {
    "objectID": "posts/optionprice/index.html#references",
    "href": "posts/optionprice/index.html#references",
    "title": "Pricing Derivatives Using Black-Scholes-Merton Model",
    "section": "References",
    "text": "References\n\nKaratzas, I., & Shreve, S. E. (1991). Brownian Motion and Stochastic Calculus.\n\nOptions, Futures, and Other Derivatives by John C. Hull\n\nArbitrage Theory in Continuous Time Book by Tomas Björk\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/statisticaltalk/index.html",
    "href": "posts/statisticaltalk/index.html",
    "title": "Some Key Statistical Concepts for Interview Prep",
    "section": "",
    "text": "In the world of data analysis and machine learning, statistics plays a vital role in making sense of the data. Whether you’re estimating parameters, testing hypotheses, or understanding relationships between variables, statistical concepts guide how we interpret data. In this post, I want to summarise and collect some fundamental statistical ideas that are quite common and asked in many data science, machine learning, and quant interviews"
  },
  {
    "objectID": "posts/statisticaltalk/index.html#basic-statistical-terminologies",
    "href": "posts/statisticaltalk/index.html#basic-statistical-terminologies",
    "title": "Some Key Statistical Concepts for Interview Prep",
    "section": "Basic Statistical Terminologies",
    "text": "Basic Statistical Terminologies\n\nThe mean\nThe mean is one of the most basic statistical concepts and represents the average value of a dataset. It’s calculated by summing all the values in a dataset and then dividing by the number of observations.\nMathematically, for a set of discrete observations \\(x_1, x_2, ..., x_n\\), the mean \\(\\mu\\) or Expected Value is defined as:\n\\[\\begin{align*}\n\\mu &=  \\frac{1}{n} \\sum_{i=1}^{n} x_i\\\\\n\\implies \\mathbb{E}[X] &= \\sum_{i=1}^{n} x_i\\mathbb{P}(X=x_i)\n\\end{align*}\\]\nFor a continuous random variable \\(X\\), the mean\n\\[\n\\mu = \\mathbb{E}[X]=\\int_{-\\infty}^{\\infty}xf_X(x)dx\n\\]\n\nwhere, \\(\\mathbb{P}(X=x)\\) is the probability mass function (pmf) and \\(f_X(x)\\) is the probability density function (pdf) of the random variable \\(X\\), depending on whether it is discrete or contineous type. The mean helps describe the central tendency of data, but it can be sensitive to outliers.\n\n\n\nVariance\n\nVariance measures the spread or dispersion of a dataset relative to its mean. It tells us how far the individual data points are from the mean. A small variance indicates that data points are clustered closely around the mean, while a large variance means they are spread out.\n\nThe formula for variance \\(\\sigma^2\\) is:\n\\[\\begin{align*}\n    \\sigma^2=Var(X)&=\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2\\\\\n    &=\\mathbb{E}\\left[(X-\\mathbb{E}[X])^2\\right]\\\\\n    &=\\mathbb{E}\\left[(X^2-2X\\mathbb{E}[X]+(\\mathbb{E}[X])^2)\\right]\\\\\n    &=\\mathbb{E}[X^2]-2\\mathbb{E}[X]\\mathbb{E}[X]+(\\mathbb{E}[X])^2\\\\\n    &=\\mathbb{E}[X^2]-(\\mathbb{E}[X])^2\\\\\n\\end{align*}\\]\nHowever, the population and sample variance formula are slightly different. For discrete observations, the sample variance is given as\n\\[ s= \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu)^2\\]\nInstead of dividing by \\(n\\) we devide by \\(n-1\\) to have the sample variance unbiased and bigger than the population variance so that it contains the true population variance.\nExamples\n\nNormal Distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) has the pdf \\(f_{X}(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]}\\)\n\nStandard Normal Distribution with mean \\(0\\) and variance \\(1\\) has the pdf \\(f_{X}(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp{\\left[-\\frac{x^2}{2}\\right]}\\)\n\nNow if \\(\\log X\\sim \\mathbfcal{N}(0,1)\\) then what is the distribution of \\(X\\)?\n\n\n\nCovariance\n\nCovariance measures how two variables move together. If the covariance is positive, the two variables tend to increase or decrease together. If negative, one variable tends to increase when the other decreases.\n\nThe formula for covariance between two variables \\(X\\) and \\(Y\\) is:\n\\[\n\\text{Cov}(X, Y) = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\mu_X)(Y_i - \\mu_Y)\n\\]\nHowever, covariance doesn’t indicate the strength of the relationship, which brings us to correlation.\n\n\nCorrelation\n\nCorrelation is a standardized measure of the relationship between two variables. It ranges from \\(-1\\) to \\(1\\), where \\(1\\) indicates a perfect positive relationship, \\(-1\\) a perfect negative relationship, and \\(0\\) no relationship.\n\nThe most common correlation metric is Pearson correlation, defined as:\n\\[\n\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\\]\nUnlike covariance, correlation gives a clearer picture of the strength and direction of a linear relationship between variables.\n\n\nP-Values and Hypothesis Testing\n\nP-values and hypothesis testing form the backbone of inferential statistics. Hypothesis testing is used to determine if a given assumption (the null hypothesis \\(H_0\\)) about a population parameter is true or not.\n\n\nThe null hypothesis \\(H_0\\) typically suggests no effect or no difference.\nThe alternative hypothesis \\(H_1\\) is the claim you want to test.\n\nThe p-value is the probability of observing a result as extreme as, or more extreme than, the one obtained, assuming the null hypothesis is true. A small p-value (usually less than 0.05) indicates that the null hypothesis is unlikely, and we may reject it in favor of the alternative hypothesis.\n\n\nMaximum Likelihood Estimation (MLE)\n\nMaximum Likelihood Estimation (MLE) is a method for estimating the parameters of a statistical model. The idea behind MLE is to find the parameter values that maximize the likelihood function, which represents the probability of observing the given data under a particular model.\n\nGiven a parameter \\(\\theta\\) and observed data \\(X\\), the likelihood function is:\n\\[\nL(\\theta | X) = P(X | \\theta)\n\\]\nMLE finds the parameter \\(\\hat{\\theta}\\) that maximizes this likelihood:\n\\[\n\\hat{\\theta} = \\arg\\max_{\\theta} L(\\theta | X)\n\\]\nMLE is widely used in statistical modeling, from simple linear regression to complex machine learning algorithms.\n\n\nMaximum A Posteriori (MAP)\nWhile MLE focuses on maximizing the likelihood, Maximum A Posteriori (MAP) estimation incorporates prior information about the parameters. MAP is rooted in Bayesian statistics, where the goal is to find the parameter that maximizes the posterior distribution.\nThe posterior is given by Bayes’ Theorem:\n\\[\nP(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{P(X)}\n\\]\nMAP finds the parameter \\(\\hat{\\theta}_{\\text{MAP}}\\) that maximizes the posterior probability:\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} P(\\theta | X)\n\\]\nUnlike MLE, MAP estimation incorporates the prior distribution \\(P(\\theta)\\), making it more robust when prior knowledge is available\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Review probabilities",
    "section": "",
    "text": "In today’s world, getting placement in data science world is highly challenging and competitive. It requires a lot of things including but not limited to basic knowledge in statistics, probability, machine learning, deep learning, and computer science. Even sometimes we face some basic problems from statistics and probability that we probably have solve long ago but forgot due to lack of practice or it’s taking longer due to rusty memory. Because, in master’s and Ph.D’s we focus on a very narrow topic and get our experties on those topics. So, it’s not a shame or humiliation if we can’t do a very simple problem even though we are capable of solving thousand time harder problems than that. It’s normal."
  },
  {
    "objectID": "posts/probability/index.html#introduction",
    "href": "posts/probability/index.html#introduction",
    "title": "Review probabilities",
    "section": "",
    "text": "In today’s world, getting placement in data science world is highly challenging and competitive. It requires a lot of things including but not limited to basic knowledge in statistics, probability, machine learning, deep learning, and computer science. Even sometimes we face some basic problems from statistics and probability that we probably have solve long ago but forgot due to lack of practice or it’s taking longer due to rusty memory. Because, in master’s and Ph.D’s we focus on a very narrow topic and get our experties on those topics. So, it’s not a shame or humiliation if we can’t do a very simple problem even though we are capable of solving thousand time harder problems than that. It’s normal."
  },
  {
    "objectID": "posts/probability/index.html#conditional-probabilities-bayess-theorem",
    "href": "posts/probability/index.html#conditional-probabilities-bayess-theorem",
    "title": "Review probabilities",
    "section": "Conditional Probabilities: Bayes’s Theorem",
    "text": "Conditional Probabilities: Bayes’s Theorem\n\nAssume two coins, one fair (i.e. equal chance of getting head and tail if tossed) and the other one is unfair and always gets head if tossed. If a coin is chosen at random and tossed six times and you get heads in all six tosses, what is the probability that you are tossing the unfair one?\nSolution:\nLet,\n\n\\(F\\) be the event the coin is fair, \\(F'\\) being the event of unfair coin and\n\n\\(H\\) be the event showing up head.\n\nWe neeed to find \\(\\mathbb{P}(F'|6H)\\), the probability that we are tossing the unfair \\(F'\\) coins given that we got 6 heads.\n\\[\\begin{align}\n     \\mathbb{P}(F'|6H)&=\\frac{\\mathbb{P}(6H|F')\\mathbb{P}(F')}{\\mathbb{P}(6H)}\n\\end{align}\\]\nHere,\n\n\\(\\mathbb{P}(F)=\\mathbb{P}(F')=\\frac{1}{2}\\), the probability of chosing a fair or unfair coin\n\\(\\mathbb{P}(6H|F)=\\left(\\frac{1}{2}\\right)^6\\), by the principle that flipping a fair coin 6 times are indpendent events, and thus the probability got multiplied\n\\(\\mathbb{P}(6H|F')=1\\), sure event, since unfair coin.\n\nSo, the total probability, \\[\\mathbb{P}(6H)=\\mathbb{P}(6H|F)\\mathbb{P}(F)+\\mathbb{P}(6H|F')\\mathbb{P}(F')=\\left(\\frac{1}{2}\\right)^6\\frac{1}{2}+1\\cdot \\frac{1}{2}\\]\nTherefore,\n\\[\\mathbb{P}(F'|6H)=\\frac{\\mathbb{P}(6H|F')\\mathbb{P}(F')}{\\mathbb{P}(6H)}=\\frac{1\\cdot\\frac{1}{2}}{\\left(\\frac{1}{2}\\right)^6\\frac{1}{2}+1\\cdot \\frac{1}{2}}\\]\nOne in thousand people\n\nYou may also like"
  },
  {
    "objectID": "posts/montecarlo1/index.html",
    "href": "posts/montecarlo1/index.html",
    "title": "Monte-Carlo Methods: PRNGs",
    "section": "",
    "text": "The Monte Carlo method is a widely used statistical technique that leverages the power of randomness to solve complex mathematical problems and simulate the behavior of various systems. It’s a method that has found applications across diverse fields, including physics, finance, engineering, and biology. In this blog post, we’ll dive deeper into the Monte Carlo method and explore the mathematics behind it, along with a discussion of random number generators like Linear Congruential Generators (LCGs) and the infamous RANDU.\n\n\n\n\nThe Monte Carlo method is based on the idea of using randomness to approximate solutions to problems that may be deterministic in nature but are too complex for analytical methods. The name “Monte Carlo” is a nod to the randomness associated with the famous casino in Monaco.  The basic principle behind the Monte Carlo method is to simulate the behavior of a system by generating random samples and using them to estimate the desired quantities. Let’s consider a mathematical problem where we need to compute an integral that does not have a straightforward analytical solution:\n\n\\[\\begin{align*}\nI &= \\int_{a}^{b} f(x) \\, dx\n\\end{align*}\\]\n\nThe Monte Carlo method approximates this integral by sampling random points \\(x_i\\) uniformly from the interval \\([a, b]\\) and evaluating the function \\(f(x)\\) at these points. The integral can then be approximated as:\n\n\\[\\begin{align*}\nI \\approx \\frac{b - a}{N} \\sum_{i=1}^{N} f(x_i)\n\\end{align*}\\]\n\nwhere \\(N\\) is the number of random samples. As \\(N\\) increases, the approximation becomes more accurate, thanks to the Law of Large Numbers.  This approach is particularly useful for high-dimensional integrals, where traditional numerical integration methods become computationally expensive or infeasible.\n\n\n\n\n\nAt the heart of the Monte Carlo method lies the generation of random numbers. In practice, most simulations do not use true random numbers but rather pseudorandom numbers generated by deterministic algorithms. These pseudorandom number generators (PRNGs) produce sequences that mimic the properties of true randomness.\n\n\n\nOne of the most commonly used PRNGs is the Linear Congruential Generator (LCG). The LCG generates a sequence of numbers \\(X_1, X_2, X_3, \\ldots\\) using the recursive relation:\n\\[\\begin{align*}\nX_{n+1} &= (aX_n + c) \\mod m\n\\end{align*}\\]\nwhere:\n\n\\(X_n\\) is the \\(n\\)-th number in the sequence.\n\\(a\\) is the multiplier.\n\\(c\\) is the increment.\n\\(m\\) is the modulus.\n\n\nThe sequence starts with an initial value \\(X_0\\), known as the seed, and the parameters \\(a\\), \\(c\\), and \\(m\\) are carefully chosen to maximize the period and quality of the generated sequence.  The quality of the LCG depends on the choice of these parameters. For instance, to achieve a full period (i.e., the sequence cycles through all possible values before repeating), the following conditions must be met:\n\n\n\\(c\\) and \\(m\\) must be relatively prime.\n\\(a - 1\\) must be divisible by all prime factors of \\(m\\).\nIf \\(m\\) is divisible by 4, then \\(a - 1\\) must also be divisible by 4.\n\nA well-known example of an LCG is the minstd_rand generator used in the C++ Standard Library, which uses \\(a = 16807\\), \\(c = 0\\), and \\(m = 2^{31} - 1\\).\n\n\n\nRANDU is an example of a poorly designed LCG that became notorious for its flaws. It is defined by the recurrence relation:\n\\[\\begin{align*}\nX_{n+1} &= (65539X_n) \\mod 2^{31}\n\\end{align*}\\]\n\nAlthough RANDU was widely used in the 1960s and 1970s due to its simplicity, it was later discovered to produce sequences with significant correlations. For example, points generated using RANDU tend to lie on a small number of planes in three-dimensional space, which can severely impact the accuracy of Monte Carlo simulations.  The generator’s flaws arise from poor parameter selection. In RANDU, the modulus \\(m = 2^{31}\\) and the multiplier \\(a = 65539\\) result in a sequence with poor distribution properties. As a consequence, RANDU’s generated numbers do not pass modern statistical tests for randomness, rendering it unsuitable for serious applications.  Let’s solve some math problems and visualize randomness.\n\n\n\n\nGiven an LCG with parameters \\(a,c,m\\), prove that\n\nwhich shows that the \\((n+k)th\\) term can be computed directly from the \\(nth\\) term.\n\n\n\nWe know from D. H. Lehmer’s linear congruential generator that\n\\[\\begin{equation}\nx_n \\equiv ax_{n-1}+c \\mod m\n\\end{equation}\\]\nwhere \\(a\\) is called the multiplier, \\(c\\) is called the shift or increment, and \\(m\\) is called the modulus of the generator. The given equation is also an LCG. We can prove this by induction method. Since \\(k\\ge 0\\) so, let \\(k=0\\). Then the given relation can be written as\n\nIf \\(k=1\\). Then the given relation can be written as\n\\[\\begin{align*}\nx_{n+1}& \\equiv ax_n+\\frac{a-1}{a-1}c \\mod m\\\\\n&\\equiv ax_n+c \\mod m\n\\end{align*}\\]\nIf \\(k=2\\). Then the given relation can be written as\n\\[\\begin{align*}\nx_{n+2}& \\equiv a^2x_n+\\frac{a^2-1}{a-1}c \\mod m\\\\\n&\\equiv a^2x_n+(a+1)c \\mod m\\\\\n&\\equiv a^2x_n+ac+c \\mod m \\\\\n&\\equiv a(ax_n+c)+c \\mod m\\\\\n&\\equiv ax_{n+1}+c \\mod m\n\\end{align*}\\]\nNow for any \\(k=p\\) where \\(p\\in \\mathbb{N}\\), \\[\\begin{align*}\nx_{n+p}& \\equiv a^px_n+\\frac{a^p-1}{a-1}c \\mod m \\\\\n\\end{align*}\\]\nNow by the method of induction, the given equation would be a lcg if it holds for any \\(k=p\\in \\mathbb{N}\\) then it must hold for \\(k=p+1\\) where \\(p\\in \\mathbb{N}\\). Now from equation (1) \\[\\begin{align*}\nx_{n+p+1} &\\equiv ax_{(n+p+1)-1}+c \\mod m\\\\\n& \\equiv ax_{n+p}+c \\mod m \\\\\n& \\equiv a(a^px_n+\\frac{a^p-1}{a-1}c) +c \\mod m\\\\\n& \\equiv a^{p+1}x_n+(a\\frac{a^p-1}{a-1}+1)c \\mod m\\\\\n& \\equiv a^{p+1}x_n+\\frac{a^{p+1}-1}{a-1}c \\mod m\\\\\n\\end{align*}\\]\nWhich proves that \\(x_{n+k}=a^kx_n+\\frac{(a^k-1)}{a-1}c (\\mod m)\\); \\((a\\ge 2, k\\ge0)\\) is an lcg such that \\((n+k)th\\) term can be computed directly from the \\(nth\\) term.\n\n\n\n(a)\nIf \\(U\\) and \\(V\\) are independently distributed random variables from the uniform distribution \\(U(0,1)\\) show that \\(U+V (\\mod 1)\\) is also \\(U(0,1)\\).\nSolution\nLet \\(Z=U+V\\) where \\(U\\) and \\(V\\) are independently distributed random variables from the uniform distribution \\(U(0,1)\\). So the minimum value that \\(Z\\) can have is \\(0\\) and the maximum value could be \\(2\\). If \\(f_U(u)\\) is the PDF of \\(U\\) and \\(f_V(v)\\) is the PDF of \\(V\\) then the PDF of \\(Z\\) can be found from the convolution of two distribution as follows \\[\\begin{align*}\n  f_Z(z)=\\int_{-\\infty}^{+\\infty}f_U(u)f_V(z-u)du=\\begin{cases}\n          z & \\text{for} \\hspace{2mm} 0 &lt; z &lt; 1\\\\\n          2-z & \\text{for} \\hspace{2mm} 1 \\le z &lt;2\\\\\n          0 & \\text{otherwise}\n         \\end{cases}\n\\end{align*}\\] Now for any \\(x\\in (0,1)\\) \\[\\begin{align*}\n  \\mathbb{P}(U+V (\\mod 1) \\le x) &= \\mathbb{P}(Z \\le x)+ \\mathbb{P}(1\\le Z \\le x+1)\\\\\n                                 &= \\int_{0}^{x} z dz +\\int_{1}^{1+x}(2-z)dz\\\\\n                                 &=x\n\\end{align*}\\]\nwhich is the CDF of a random variable distributed \\(U(0,1)\\)\n(b)\nA random number generator is designed by\n\nwhere \\(X_0=0, Y_0=1, X_{n+1}=(9X_n+3) \\mod 8\\) and \\(Y_{n+1}=3Y_n \\mod 7\\) for \\(n=0,1,2,\\cdots\\). Calculate \\(R_0,R_1,R_2, \\cdots , R_5.\\). What is the period of the generator \\(\\{R_n\\}\\)?\nSolution\n\nimport numpy as np\nimport pandas as pd\n\ndef rand_gen(n):\n    RN = np.zeros(n + 1)\n    x = np.zeros(n + 1, dtype=int)\n    y = np.zeros(n + 1, dtype=int)\n\n    # Initial values\n    x[0] = 0\n    y[0] = 1\n    RN[0] = (x[0] / 8 + y[0] / 7) % 1\n\n    # Iterative process\n    for i in range(n):\n        x[i + 1] = (9 * x[i] + 3) % 8\n        y[i + 1] = (3 * y[i]) % 7\n        RN[i + 1] = (x[i + 1] / 8 + y[i + 1] / 7) % 1\n\n    return pd.DataFrame({\"X_values\": x, \"Y_values\": y, \"R_values\": RN})\n\n# Generate the first 4 values\nprint(rand_gen(4))\n\n   X_values  Y_values  R_values\n0         0         1  0.142857\n1         3         3  0.803571\n2         6         2  0.035714\n3         1         6  0.982143\n4         4         4  0.071429\n\n\nSo the unique values are\n\nunique_values = rand_gen(2**10)[\"R_values\"].unique()\nfor i in range(len(unique_values)):\n    print(i,' ',np.round(unique_values[i],4))\n\n0   0.1429\n1   0.8036\n2   0.0357\n3   0.9821\n4   0.0714\n5   0.5893\n6   0.3929\n7   0.0536\n8   0.2857\n9   0.2321\n10   0.3214\n11   0.8393\n12   0.6429\n13   0.3036\n14   0.5357\n15   0.4821\n16   0.5714\n17   0.0893\n18   0.8929\n19   0.5536\n20   0.7857\n21   0.7321\n22   0.8214\n23   0.3393\n\n\nSo from the above data we can see that the period is \\(24\\).\n\n\n\nWrite a code that would implement RANDU. For debugging purpose print \\(x_{1000}\\) when the seed is \\(x_0=1\\)\n(a)\nUsing RANDU generate \\(u_1,u_2,\\cdots, u_{20,002}\\) where \\(u=\\frac{x_n}{M}\\). For all triplets in your sequence, \\(u_i, u_{i+1}, u_{i+2}\\), in which \\(0.5\\le u_{i+1} \\le 0.51\\) plot \\(u_i\\) versus \\(u_{i+2}\\). Comment on the pattern of your scatterplot.\n\nimport matplotlib.pyplot as plt\n\n# Set seed\nseed = 1.0\nN = 20002\n\n# RANDU function\ndef RANDU():\n    global seed\n    seed = ((2**16 + 3) * seed) % (2**31)\n    return round(seed / (2**31), 6)\n\n# Generate random numbers\nA = np.zeros((N, 3))\n\nfor i in range(N):\n    A[i, 0] = RANDU()\n    A[i, 1] = RANDU()\n    A[i, 2] = RANDU()\n\n# Convert to DataFrame\nB = pd.DataFrame(A, columns=[\"V1\", \"V2\", \"V3\"])\n\n# Filter data where V2 is between 0.5 and 0.51\nC = B[(B[\"V2\"] &gt;= 0.5) & (B[\"V2\"] &lt;= 0.51)]\n\n# Set the background color\nplt.figure(figsize=(8, 6), facecolor=\"#f4f4f4\")\n\n# Scatter plot\nplt.scatter(C[\"V1\"], C[\"V3\"], color=\"blue\", alpha=0.6, edgecolors=\"black\")\nplt.xlabel(\"u_i\")\nplt.ylabel(\"u_(i+3)\")\nplt.title(\"Scatter Plot of Filtered Random Numbers\")\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n(b)\nGenerate a sequence of lenght 1002. Use a program that plots points in 3 dimensions and rotates the axes to rotate the points until you can see the 15 planes.\n\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\n\n# Set seed\nseed = 1.0\nN = 1002\n\n# RANDU function\ndef RANDU():\n    global seed\n    seed = ((2**16 + 3) * seed) % (2**31)\n    return round(seed / (2**31), 6)\n\n# Generate random numbers\nA = np.zeros((N, 3))\n\nfor i in range(N):\n    A[i, 0] = RANDU()\n    A[i, 1] = RANDU()\n    A[i, 2] = RANDU()\n\n# Convert to DataFrame\nB = pd.DataFrame(A, columns=[\"V1\", \"V2\", \"V3\"])\n\n# Create 3D Scatter Plot using Plotly\nfig = go.Figure()\n\nfig.add_trace(go.Scatter3d(\n    x=B[\"V1\"], y=B[\"V2\"], z=B[\"V3\"],\n    mode='markers',\n    marker=dict(\n        size=4,\n        color=np.linspace(0, 1, N),\n        colorscale='rainbow',\n        opacity=0.8\n    )\n))\n\n# Set background color\nfig.update_layout(\n    title=\"3D Random Number Visualization\",\n    scene=dict(\n        xaxis_title=\"V1\",\n        yaxis_title=\"V2\",\n        zaxis_title=\"V3\",\n        bgcolor=\"#f4f4f4\"\n    ),\n    margin=dict(l=0, r=0, b=0, t=40),\n)\n\n# Show interactive 3D plot\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\nA circle with radius \\(r=1\\) has the area \\(A=\\pi r^2= \\pi\\) and a square with length \\(l=1\\) has the area \\(B=1\\). Now if we consider the following situation, where a quarter of a unit circle is inscribed inside a unit square like this\n\n\nCode\nnum_points = 100\npts = np.random.rand(num_points,2)\n\nfig, axes = plt.subplots()\ntheta = np.linspace(0, np.pi/2,100)\nx = np.cos(theta)\ny = np.sin(theta)\n\naxes.plot(x, y, 'b')\naxes.plot([0,1],[0,0],'k')\naxes.plot([1,1],[0,1],'k')\naxes.plot([1,0],[1,1],'k')\naxes.plot([0,0],[1,0],'k')\n\nfor p in pts:\n    if p[0]**2+p[1]**2 &lt;=1:\n        axes.plot(p[0], p[1], 'go')\n    else:\n        axes.plot(p[0], p[1], 'ro')\naxes.set_aspect('equal')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nwe get,\n\\[\\begin{align*}\n\\frac{\\text{Area of the quarter of a unit circle: C}}{\\text{Area of a unit square: S}}&= \\frac{\\frac{\\pi}{4}}{1}=\\frac{\\pi}{4}\\hspace{4mm}\\implies \\pi = \\frac{4C}{S}\n\\end{align*}\\]\n\nThe above relation tells us some interesting fact. If we uniformly create as many points as possible inside the square then the number of points inside the circle will be approximately 4 times the number of the points outside the circular region.\n\n\ndef monte_carlo_pi(n):\n    inside_circle = 0\n\n    for _ in range(n):\n        x = np.random.uniform(0, 1)\n        y = np.random.uniform(0, 1)\n        if x**2 + y**2 &lt;= 1:\n            inside_circle += 1\n\n    pi_estimate = (inside_circle / n) * 4\n    return pi_estimate\n\n# Number of points\nn = 10000\nprint(f\"Monte Carlo estimated value of π from {n} points = {monte_carlo_pi(n):.6f}\")\n\nMonte Carlo estimated value of π from 10000 points = 3.131200\n\n\nThat’s all for this post."
  },
  {
    "objectID": "posts/montecarlo1/index.html#introduction",
    "href": "posts/montecarlo1/index.html#introduction",
    "title": "Monte-Carlo Methods: PRNGs",
    "section": "",
    "text": "The Monte Carlo method is a widely used statistical technique that leverages the power of randomness to solve complex mathematical problems and simulate the behavior of various systems. It’s a method that has found applications across diverse fields, including physics, finance, engineering, and biology. In this blog post, we’ll dive deeper into the Monte Carlo method and explore the mathematics behind it, along with a discussion of random number generators like Linear Congruential Generators (LCGs) and the infamous RANDU.\n\n\n\n\nThe Monte Carlo method is based on the idea of using randomness to approximate solutions to problems that may be deterministic in nature but are too complex for analytical methods. The name “Monte Carlo” is a nod to the randomness associated with the famous casino in Monaco.  The basic principle behind the Monte Carlo method is to simulate the behavior of a system by generating random samples and using them to estimate the desired quantities. Let’s consider a mathematical problem where we need to compute an integral that does not have a straightforward analytical solution:\n\n\\[\\begin{align*}\nI &= \\int_{a}^{b} f(x) \\, dx\n\\end{align*}\\]\n\nThe Monte Carlo method approximates this integral by sampling random points \\(x_i\\) uniformly from the interval \\([a, b]\\) and evaluating the function \\(f(x)\\) at these points. The integral can then be approximated as:\n\n\\[\\begin{align*}\nI \\approx \\frac{b - a}{N} \\sum_{i=1}^{N} f(x_i)\n\\end{align*}\\]\n\nwhere \\(N\\) is the number of random samples. As \\(N\\) increases, the approximation becomes more accurate, thanks to the Law of Large Numbers.  This approach is particularly useful for high-dimensional integrals, where traditional numerical integration methods become computationally expensive or infeasible.\n\n\n\n\n\nAt the heart of the Monte Carlo method lies the generation of random numbers. In practice, most simulations do not use true random numbers but rather pseudorandom numbers generated by deterministic algorithms. These pseudorandom number generators (PRNGs) produce sequences that mimic the properties of true randomness.\n\n\n\nOne of the most commonly used PRNGs is the Linear Congruential Generator (LCG). The LCG generates a sequence of numbers \\(X_1, X_2, X_3, \\ldots\\) using the recursive relation:\n\\[\\begin{align*}\nX_{n+1} &= (aX_n + c) \\mod m\n\\end{align*}\\]\nwhere:\n\n\\(X_n\\) is the \\(n\\)-th number in the sequence.\n\\(a\\) is the multiplier.\n\\(c\\) is the increment.\n\\(m\\) is the modulus.\n\n\nThe sequence starts with an initial value \\(X_0\\), known as the seed, and the parameters \\(a\\), \\(c\\), and \\(m\\) are carefully chosen to maximize the period and quality of the generated sequence.  The quality of the LCG depends on the choice of these parameters. For instance, to achieve a full period (i.e., the sequence cycles through all possible values before repeating), the following conditions must be met:\n\n\n\\(c\\) and \\(m\\) must be relatively prime.\n\\(a - 1\\) must be divisible by all prime factors of \\(m\\).\nIf \\(m\\) is divisible by 4, then \\(a - 1\\) must also be divisible by 4.\n\nA well-known example of an LCG is the minstd_rand generator used in the C++ Standard Library, which uses \\(a = 16807\\), \\(c = 0\\), and \\(m = 2^{31} - 1\\).\n\n\n\nRANDU is an example of a poorly designed LCG that became notorious for its flaws. It is defined by the recurrence relation:\n\\[\\begin{align*}\nX_{n+1} &= (65539X_n) \\mod 2^{31}\n\\end{align*}\\]\n\nAlthough RANDU was widely used in the 1960s and 1970s due to its simplicity, it was later discovered to produce sequences with significant correlations. For example, points generated using RANDU tend to lie on a small number of planes in three-dimensional space, which can severely impact the accuracy of Monte Carlo simulations.  The generator’s flaws arise from poor parameter selection. In RANDU, the modulus \\(m = 2^{31}\\) and the multiplier \\(a = 65539\\) result in a sequence with poor distribution properties. As a consequence, RANDU’s generated numbers do not pass modern statistical tests for randomness, rendering it unsuitable for serious applications.  Let’s solve some math problems and visualize randomness.\n\n\n\n\nGiven an LCG with parameters \\(a,c,m\\), prove that\n\nwhich shows that the \\((n+k)th\\) term can be computed directly from the \\(nth\\) term.\n\n\n\nWe know from D. H. Lehmer’s linear congruential generator that\n\\[\\begin{equation}\nx_n \\equiv ax_{n-1}+c \\mod m\n\\end{equation}\\]\nwhere \\(a\\) is called the multiplier, \\(c\\) is called the shift or increment, and \\(m\\) is called the modulus of the generator. The given equation is also an LCG. We can prove this by induction method. Since \\(k\\ge 0\\) so, let \\(k=0\\). Then the given relation can be written as\n\nIf \\(k=1\\). Then the given relation can be written as\n\\[\\begin{align*}\nx_{n+1}& \\equiv ax_n+\\frac{a-1}{a-1}c \\mod m\\\\\n&\\equiv ax_n+c \\mod m\n\\end{align*}\\]\nIf \\(k=2\\). Then the given relation can be written as\n\\[\\begin{align*}\nx_{n+2}& \\equiv a^2x_n+\\frac{a^2-1}{a-1}c \\mod m\\\\\n&\\equiv a^2x_n+(a+1)c \\mod m\\\\\n&\\equiv a^2x_n+ac+c \\mod m \\\\\n&\\equiv a(ax_n+c)+c \\mod m\\\\\n&\\equiv ax_{n+1}+c \\mod m\n\\end{align*}\\]\nNow for any \\(k=p\\) where \\(p\\in \\mathbb{N}\\), \\[\\begin{align*}\nx_{n+p}& \\equiv a^px_n+\\frac{a^p-1}{a-1}c \\mod m \\\\\n\\end{align*}\\]\nNow by the method of induction, the given equation would be a lcg if it holds for any \\(k=p\\in \\mathbb{N}\\) then it must hold for \\(k=p+1\\) where \\(p\\in \\mathbb{N}\\). Now from equation (1) \\[\\begin{align*}\nx_{n+p+1} &\\equiv ax_{(n+p+1)-1}+c \\mod m\\\\\n& \\equiv ax_{n+p}+c \\mod m \\\\\n& \\equiv a(a^px_n+\\frac{a^p-1}{a-1}c) +c \\mod m\\\\\n& \\equiv a^{p+1}x_n+(a\\frac{a^p-1}{a-1}+1)c \\mod m\\\\\n& \\equiv a^{p+1}x_n+\\frac{a^{p+1}-1}{a-1}c \\mod m\\\\\n\\end{align*}\\]\nWhich proves that \\(x_{n+k}=a^kx_n+\\frac{(a^k-1)}{a-1}c (\\mod m)\\); \\((a\\ge 2, k\\ge0)\\) is an lcg such that \\((n+k)th\\) term can be computed directly from the \\(nth\\) term.\n\n\n\n(a)\nIf \\(U\\) and \\(V\\) are independently distributed random variables from the uniform distribution \\(U(0,1)\\) show that \\(U+V (\\mod 1)\\) is also \\(U(0,1)\\).\nSolution\nLet \\(Z=U+V\\) where \\(U\\) and \\(V\\) are independently distributed random variables from the uniform distribution \\(U(0,1)\\). So the minimum value that \\(Z\\) can have is \\(0\\) and the maximum value could be \\(2\\). If \\(f_U(u)\\) is the PDF of \\(U\\) and \\(f_V(v)\\) is the PDF of \\(V\\) then the PDF of \\(Z\\) can be found from the convolution of two distribution as follows \\[\\begin{align*}\n  f_Z(z)=\\int_{-\\infty}^{+\\infty}f_U(u)f_V(z-u)du=\\begin{cases}\n          z & \\text{for} \\hspace{2mm} 0 &lt; z &lt; 1\\\\\n          2-z & \\text{for} \\hspace{2mm} 1 \\le z &lt;2\\\\\n          0 & \\text{otherwise}\n         \\end{cases}\n\\end{align*}\\] Now for any \\(x\\in (0,1)\\) \\[\\begin{align*}\n  \\mathbb{P}(U+V (\\mod 1) \\le x) &= \\mathbb{P}(Z \\le x)+ \\mathbb{P}(1\\le Z \\le x+1)\\\\\n                                 &= \\int_{0}^{x} z dz +\\int_{1}^{1+x}(2-z)dz\\\\\n                                 &=x\n\\end{align*}\\]\nwhich is the CDF of a random variable distributed \\(U(0,1)\\)\n(b)\nA random number generator is designed by\n\nwhere \\(X_0=0, Y_0=1, X_{n+1}=(9X_n+3) \\mod 8\\) and \\(Y_{n+1}=3Y_n \\mod 7\\) for \\(n=0,1,2,\\cdots\\). Calculate \\(R_0,R_1,R_2, \\cdots , R_5.\\). What is the period of the generator \\(\\{R_n\\}\\)?\nSolution\n\nimport numpy as np\nimport pandas as pd\n\ndef rand_gen(n):\n    RN = np.zeros(n + 1)\n    x = np.zeros(n + 1, dtype=int)\n    y = np.zeros(n + 1, dtype=int)\n\n    # Initial values\n    x[0] = 0\n    y[0] = 1\n    RN[0] = (x[0] / 8 + y[0] / 7) % 1\n\n    # Iterative process\n    for i in range(n):\n        x[i + 1] = (9 * x[i] + 3) % 8\n        y[i + 1] = (3 * y[i]) % 7\n        RN[i + 1] = (x[i + 1] / 8 + y[i + 1] / 7) % 1\n\n    return pd.DataFrame({\"X_values\": x, \"Y_values\": y, \"R_values\": RN})\n\n# Generate the first 4 values\nprint(rand_gen(4))\n\n   X_values  Y_values  R_values\n0         0         1  0.142857\n1         3         3  0.803571\n2         6         2  0.035714\n3         1         6  0.982143\n4         4         4  0.071429\n\n\nSo the unique values are\n\nunique_values = rand_gen(2**10)[\"R_values\"].unique()\nfor i in range(len(unique_values)):\n    print(i,' ',np.round(unique_values[i],4))\n\n0   0.1429\n1   0.8036\n2   0.0357\n3   0.9821\n4   0.0714\n5   0.5893\n6   0.3929\n7   0.0536\n8   0.2857\n9   0.2321\n10   0.3214\n11   0.8393\n12   0.6429\n13   0.3036\n14   0.5357\n15   0.4821\n16   0.5714\n17   0.0893\n18   0.8929\n19   0.5536\n20   0.7857\n21   0.7321\n22   0.8214\n23   0.3393\n\n\nSo from the above data we can see that the period is \\(24\\).\n\n\n\nWrite a code that would implement RANDU. For debugging purpose print \\(x_{1000}\\) when the seed is \\(x_0=1\\)\n(a)\nUsing RANDU generate \\(u_1,u_2,\\cdots, u_{20,002}\\) where \\(u=\\frac{x_n}{M}\\). For all triplets in your sequence, \\(u_i, u_{i+1}, u_{i+2}\\), in which \\(0.5\\le u_{i+1} \\le 0.51\\) plot \\(u_i\\) versus \\(u_{i+2}\\). Comment on the pattern of your scatterplot.\n\nimport matplotlib.pyplot as plt\n\n# Set seed\nseed = 1.0\nN = 20002\n\n# RANDU function\ndef RANDU():\n    global seed\n    seed = ((2**16 + 3) * seed) % (2**31)\n    return round(seed / (2**31), 6)\n\n# Generate random numbers\nA = np.zeros((N, 3))\n\nfor i in range(N):\n    A[i, 0] = RANDU()\n    A[i, 1] = RANDU()\n    A[i, 2] = RANDU()\n\n# Convert to DataFrame\nB = pd.DataFrame(A, columns=[\"V1\", \"V2\", \"V3\"])\n\n# Filter data where V2 is between 0.5 and 0.51\nC = B[(B[\"V2\"] &gt;= 0.5) & (B[\"V2\"] &lt;= 0.51)]\n\n# Set the background color\nplt.figure(figsize=(8, 6), facecolor=\"#f4f4f4\")\n\n# Scatter plot\nplt.scatter(C[\"V1\"], C[\"V3\"], color=\"blue\", alpha=0.6, edgecolors=\"black\")\nplt.xlabel(\"u_i\")\nplt.ylabel(\"u_(i+3)\")\nplt.title(\"Scatter Plot of Filtered Random Numbers\")\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n(b)\nGenerate a sequence of lenght 1002. Use a program that plots points in 3 dimensions and rotates the axes to rotate the points until you can see the 15 planes.\n\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\n\n# Set seed\nseed = 1.0\nN = 1002\n\n# RANDU function\ndef RANDU():\n    global seed\n    seed = ((2**16 + 3) * seed) % (2**31)\n    return round(seed / (2**31), 6)\n\n# Generate random numbers\nA = np.zeros((N, 3))\n\nfor i in range(N):\n    A[i, 0] = RANDU()\n    A[i, 1] = RANDU()\n    A[i, 2] = RANDU()\n\n# Convert to DataFrame\nB = pd.DataFrame(A, columns=[\"V1\", \"V2\", \"V3\"])\n\n# Create 3D Scatter Plot using Plotly\nfig = go.Figure()\n\nfig.add_trace(go.Scatter3d(\n    x=B[\"V1\"], y=B[\"V2\"], z=B[\"V3\"],\n    mode='markers',\n    marker=dict(\n        size=4,\n        color=np.linspace(0, 1, N),\n        colorscale='rainbow',\n        opacity=0.8\n    )\n))\n\n# Set background color\nfig.update_layout(\n    title=\"3D Random Number Visualization\",\n    scene=dict(\n        xaxis_title=\"V1\",\n        yaxis_title=\"V2\",\n        zaxis_title=\"V3\",\n        bgcolor=\"#f4f4f4\"\n    ),\n    margin=dict(l=0, r=0, b=0, t=40),\n)\n\n# Show interactive 3D plot\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\nA circle with radius \\(r=1\\) has the area \\(A=\\pi r^2= \\pi\\) and a square with length \\(l=1\\) has the area \\(B=1\\). Now if we consider the following situation, where a quarter of a unit circle is inscribed inside a unit square like this\n\n\nCode\nnum_points = 100\npts = np.random.rand(num_points,2)\n\nfig, axes = plt.subplots()\ntheta = np.linspace(0, np.pi/2,100)\nx = np.cos(theta)\ny = np.sin(theta)\n\naxes.plot(x, y, 'b')\naxes.plot([0,1],[0,0],'k')\naxes.plot([1,1],[0,1],'k')\naxes.plot([1,0],[1,1],'k')\naxes.plot([0,0],[1,0],'k')\n\nfor p in pts:\n    if p[0]**2+p[1]**2 &lt;=1:\n        axes.plot(p[0], p[1], 'go')\n    else:\n        axes.plot(p[0], p[1], 'ro')\naxes.set_aspect('equal')\nplt.gca().set_facecolor('#f4f4f4') \nplt.gcf().patch.set_facecolor('#f4f4f4')\nplt.show()\n\n\n\n\n\n\n\n\n\nwe get,\n\\[\\begin{align*}\n\\frac{\\text{Area of the quarter of a unit circle: C}}{\\text{Area of a unit square: S}}&= \\frac{\\frac{\\pi}{4}}{1}=\\frac{\\pi}{4}\\hspace{4mm}\\implies \\pi = \\frac{4C}{S}\n\\end{align*}\\]\n\nThe above relation tells us some interesting fact. If we uniformly create as many points as possible inside the square then the number of points inside the circle will be approximately 4 times the number of the points outside the circular region.\n\n\ndef monte_carlo_pi(n):\n    inside_circle = 0\n\n    for _ in range(n):\n        x = np.random.uniform(0, 1)\n        y = np.random.uniform(0, 1)\n        if x**2 + y**2 &lt;= 1:\n            inside_circle += 1\n\n    pi_estimate = (inside_circle / n) * 4\n    return pi_estimate\n\n# Number of points\nn = 10000\nprint(f\"Monte Carlo estimated value of π from {n} points = {monte_carlo_pi(n):.6f}\")\n\nMonte Carlo estimated value of π from 10000 points = 3.131200\n\n\nThat’s all for this post."
  },
  {
    "objectID": "posts/montecarlo1/index.html#reference",
    "href": "posts/montecarlo1/index.html#reference",
    "title": "Monte-Carlo Methods: PRNGs",
    "section": "Reference",
    "text": "Reference\n\nOkten, G. (1999). Contributions to the theory of Monte Carlo and quasi-Monte Carlo methods. Universal-Publishers.\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/lln/index.html",
    "href": "posts/lln/index.html",
    "title": "Understanding Law of Large Numbers and Central Limit Theorem",
    "section": "",
    "text": "Statistical inference is a fundamental part of data science and machine learning. Two of the most important theorems in probability theory that form the backbone of many statistical methods are the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT). It’s not uncommon to mix-up with these two concepts often."
  },
  {
    "objectID": "posts/lln/index.html#the-law-of-large-numbers-lln",
    "href": "posts/lln/index.html#the-law-of-large-numbers-lln",
    "title": "Understanding Law of Large Numbers and Central Limit Theorem",
    "section": "The Law of Large Numbers (LLN)",
    "text": "The Law of Large Numbers (LLN)\n\nIntuition Behind LLN\n\nThe Law of Large Numbers states that as the sample size increases, the sample mean approaches the population mean. In other words, with more observations, the average of the sample becomes a better estimate of the true average of the population.\n\n\n\nMathematical Definition\nLet \\(X_1, X_2, X_3, \\dots, X_n\\) be a sequence of independent and identically distributed (i.i.d.) random variables with an expected value \\(\\mu\\). The sample mean is given by:\n\\[\n\\bar{X_n} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n\\] According to LLN: \\[\n\\bar{X_n} \\to \\mu \\quad \\text{as } n \\to \\infty\n\\]\nThis means that for a sufficiently large \\(n\\), \\(\\bar{X_n}\\) will be very close to \\(\\mu\\). That is\n\\[\n\\lim_{n\\to \\infty} \\bar{X}_n = \\lim_{n\\to \\infty} \\frac{1}{n} \\sum_{i=1}^{n} X_i = \\mu\n\\]\n\n\nVisualization\nSay, we have a population of 150,000 male in a country called VSA (a hypothetical country)\n\n\nCode\nimport numpy as np\n\nheights = np.random.randint(low=90, high=190, size=150000)\nmean_height = np.mean(heights)\n\n\nand the true mean/average height of men is np.float64(139.57). We want to see how varying the sample size affect the sample mean and variances.\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\n\nsample_sizes = [50, 100, 1000, 5000]\nnumber_of_studies = 10000\n# Create subplots\nfig, axes = plt.subplots(2, 2, figsize=(8.3, 5.5))\naxes = axes.ravel()\n\nfor i, n in enumerate(sample_sizes):\n    sample_means = np.zeros(number_of_studies)\n    for j in range(number_of_studies):\n        sample = np.random.choice(heights, size=n, replace=True)\n        sample_means[j] = np.mean(sample)\n    \n    # Plot histogram\n    axes[i].hist(sample_means, bins=35, density=True, alpha=0.7, color='blue')\n    ymin, ymax = axes[i].get_ylim()\n    axes[i].plot([mean_height, mean_height], [ymin, ymax], color='red', linestyle='-',linewidth=3, label='True Mean')\n    axes[i].set_title(f\"Sample Size: {n}\")\n    axes[i].set_xlabel(\"Sample Mean\")\n    axes[i].set_ylabel(\"Density\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.savefig('lln.png')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFrom the top-left figure, when the sample size is 50, the sample mean varies from 130 to 150 where the true population mean is np.float64(139.57), similarly in the 4th figure (bottom-left), when the sample size is 5000, the mean varies from 138 to 141. Thus, if we increase the sample size, sufficiently large, then the sample mean is approximately equal to the population mean. This is what LLN is!\n\n\n\nTypes of LLN\nThere are two forms of the Law of Large Numbers:\n\nWeak Law of Large Numbers (WLLN): Convergence of the sample mean to the population mean happens in probability.\nStrong Law of Large Numbers (SLLN): Convergence happens almost surely, meaning with probability 1.\n\n\n\nWhy is LLN Important?\nLLN justifies the use of sample means in estimation problems. For example, if we want to estimate the average income of a country, we don’t need to survey the entire population; we can take a large enough sample, and the sample mean will approximate the true mean."
  },
  {
    "objectID": "posts/lln/index.html#the-central-limit-theorem-clt",
    "href": "posts/lln/index.html#the-central-limit-theorem-clt",
    "title": "Understanding Law of Large Numbers and Central Limit Theorem",
    "section": "The Central Limit Theorem (CLT)",
    "text": "The Central Limit Theorem (CLT)\n\nIntuition Behind CLT\n\nWhile the Law of Large Numbers tells us that sample means converge to the population mean, the Central Limit Theorem goes further. It states that the distribution of the sample mean follows a normal distribution, regardless of the shape of the population distribution, provided the sample size is large enough.\n\n\n\nMathematical Definition\nLet \\(X_1, X_2, \\dots, X_n\\) be i.i.d. random variables with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Define the sample mean: \\[\n\\bar{X_n} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n\\]\nThen, as \\(n\\) increases, the distribution of \\(\\bar{X_n}\\) approaches a normal distribution: \\[\n\\frac{\\bar{X_n} - \\mu}{\\sigma / \\sqrt{n}} \\approx N(0,1)\n\\]\nThis means that if we standardize \\(\\bar{X_n}\\), it follows a standard normal distribution (mean 0, variance 1) when \\(n\\) is large.\n\n\nVisualization\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Define population distributions\ndef generate_population(dist_type, size=1000000):\n    if dist_type == \"uniform\":\n        return np.random.uniform(0, 100, size)\n    elif dist_type == \"exponential\":\n        return np.random.exponential(scale=10, size=size)\n    elif dist_type == \"binomial\":\n        return np.random.binomial(n=10, p=0.5, size=size)\n    else:\n        raise ValueError(\"Unknown distribution type\")\n\n# Function to simulate sample means\ndef sample_means_experiment(population, sample_size, num_samples=10000):\n    sample_means = np.zeros(num_samples)\n    for i in range(num_samples):\n        sample = np.random.choice(population, size=sample_size, replace=True)\n        sample_means[i] = np.mean(sample)\n    return sample_means\n\n# Define sample sizes\nsample_sizes = [5, 100, 1000]\n\n# Define distributions to test\ndistributions = [\"uniform\", \"exponential\", \"binomial\"]\n\n# Plot CLT effect for different distributions\nfig, axes = plt.subplots(len(distributions), len(sample_sizes), figsize=(8, 7.5))\nfig.suptitle(\"Central Limit Theorem: Sample Mean Distributions\", fontsize=16, fontweight='bold')\n\nfor i, dist_type in enumerate(distributions):\n    population = generate_population(dist_type)\n    true_mean = np.mean(population)\n    \n    for j, n in enumerate(sample_sizes):\n        sample_means = sample_means_experiment(population, n)\n\n        # Plot histogram of sample means\n        sns.histplot(sample_means, bins=20, kde=True, ax=axes[i, j], color='blue')\n        ymin, ymax = axes[i, j].get_ylim()\n        axes[i, j].plot([true_mean, true_mean], [ymin, ymax], 'r-', linewidth=2, label=\"True Mean\")\n        \n        axes[i, j].set_title(f\"{dist_type.capitalize()} Dist, n = {n}\")\n        axes[i, j].set_xlabel(\"Sample Mean\")\n        axes[i, j].set_ylabel(\"Density\")\n\nplt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout for title\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWhy is CLT Important?\n\nAllows Statistical Inference: Even if the population distribution is unknown or non-normal, we can still use normal-based statistical techniques when working with large samples.\nEnables Hypothesis Testing and Confidence Intervals: Many inferential statistics methods assume normality. CLT ensures that these methods work for large enough samples.\nMakes Sampling Practical: Without CLT, we would need to know the entire population distribution to make inferences.\n\n\n\nExample\n\nSuppose we have a population where the income distribution is highly skewed. If we take small samples, their distributions may also be skewed. However, as the sample size increases (e.g., \\(n &gt; 30\\)), the distribution of sample means will look more like a normal distribution, allowing us to apply normal-based statistical methods."
  },
  {
    "objectID": "posts/lln/index.html#relationship-between-lln-and-clt",
    "href": "posts/lln/index.html#relationship-between-lln-and-clt",
    "title": "Understanding Law of Large Numbers and Central Limit Theorem",
    "section": "Relationship Between LLN and CLT",
    "text": "Relationship Between LLN and CLT\nWhile both the Law of Large Numbers and the Central Limit Theorem deal with large samples, they serve different purposes:\n\nLLN guarantees that the sample mean converges to the true population mean as the sample size increases.\nCLT ensures that the distribution of the sample mean follows a normal distribution when the sample size is sufficiently large.\n\nIn short, LLN helps us estimate population parameters accurately, while CLT helps us conduct statistical inference using normal approximations."
  },
  {
    "objectID": "posts/lln/index.html#conclusion",
    "href": "posts/lln/index.html#conclusion",
    "title": "Understanding Law of Large Numbers and Central Limit Theorem",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe Law of Large Numbers and the Central Limit Theorem are two of the most fundamental theorems in probability and statistics. LLN reassures us that as we collect more data, our sample mean becomes a reliable estimate of the population mean. CLT, on the other hand, enables powerful statistical techniques by ensuring that sample means follow a normal distribution, even when the underlying population is not normal.  Understanding these concepts is essential for data science, as they form the basis for many machine learning and statistical inference methods. Whether you are estimating a population parameter, conducting hypothesis tests, or building predictive models, LLN and CLT provide the theoretical foundation for making reliable decisions based on data.\n\n\n\nFurther Reading\n\n“Introduction to Probability” by Joseph K. Blitzstein and Jessica Hwang\n“The Elements of Statistical Learning” by Hastie, Tibshirani, and Friedman\nAny introductory statistics textbook covering probability theory\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "posts/corrandreg/index.html",
    "href": "posts/corrandreg/index.html",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "",
    "text": "Correlation and regression are two fundamental concepts in statistics, often used to study relationships between variables. While correlation measures the strength and direction of a linear relationship between two variables, regression goes further by modeling the relationship to predict or explain one variable based on another. This blog explores the mathematical underpinnings of both concepts, illustrating their significance in data analysis."
  },
  {
    "objectID": "posts/corrandreg/index.html#introduction",
    "href": "posts/corrandreg/index.html#introduction",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "",
    "text": "Correlation and regression are two fundamental concepts in statistics, often used to study relationships between variables. While correlation measures the strength and direction of a linear relationship between two variables, regression goes further by modeling the relationship to predict or explain one variable based on another. This blog explores the mathematical underpinnings of both concepts, illustrating their significance in data analysis."
  },
  {
    "objectID": "posts/corrandreg/index.html#correlation-analysis",
    "href": "posts/corrandreg/index.html#correlation-analysis",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nTo better explain, we will use the following hypothetical stock data of 10 companies with stock price and their corresponding proportion in the portfolio.\n\n\nCode\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Stock': ['Apple', 'Citi', 'MS', 'WF', 'GS', 'Google', 'Amazon', 'Tesla', 'Toyota', 'SPY'],\n    'StockPrice': [2.11, 2.42, 2.52, 3.21, 3.62, 3.86, 4.13, 4.27, 4.51, 5.01], \n    'Portfolio': [2.12, 2.16, 2.51, 2.65, 3.62, 3.15, 4.32, 3.31, 4.18, 4.45]\n})\n\ndf.set_index('Stock', inplace=True)\n\ndf.T\n\n\n\n\n\n\n\n\nStock\nApple\nCiti\nMS\nWF\nGS\nGoogle\nAmazon\nTesla\nToyota\nSPY\n\n\n\n\nStockPrice\n2.11\n2.42\n2.52\n3.21\n3.62\n3.86\n4.13\n4.27\n4.51\n5.01\n\n\nPortfolio\n2.12\n2.16\n2.51\n2.65\n3.62\n3.15\n4.32\n3.31\n4.18\n4.45\n\n\n\n\n\n\n\nThe scatterplot of the data looks like this\n\n\nCode\nfrom mywebstyle import plot_style\nplot_style('#f4f4f4')\nimport matplotlib.pyplot as plt\nplt.scatter(df.StockPrice, df.Portfolio, color='red')\nplt.xlabel('Stock Price')\nplt.ylabel('Portfolio')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see from the graph that there appears to be a linear relationship between the \\(x\\) and \\(y\\) values in this case. To find the relationship mathematically we define the followings\n\\[\\begin{align*}\nS_{xx}& = \\sum (x_i-\\bar{x})^2 = \\sum (x_i^2-2x_i\\bar{x}+\\bar{x}^2)\\\\\n& = \\sum x_i^2 - 2\\bar{x}\\sum x_i + \\sum \\bar{x}^2= \\sum x_i^2 - 2\\bar{x} n \\bar{x} + n \\bar{x}^2 = \\sum x_i ^2 - n \\bar{x}^2\n\\end{align*}\\]\nSimilarly, \\[\\begin{align*}\nS_{yy}& = \\sum (y_i-\\bar{y})^2=\\sum y_i ^2 - n \\bar{y}^2\\\\\nS_{xy} & = \\sum (x_i-\\bar{x})^2 \\sum (y_i-\\bar{y})^2 = \\sum x_iy_i -n \\bar{xy}\n\\end{align*}\\]\nThe sample correlation coefficient \\(r\\) is then given as\n\\[\nr = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}} = \\frac{\\sum x_i ^2 - n \\bar{x}^2}{\\sqrt{\\left(\\sum x_i ^2 - n \\bar{x}^2\\right)\\left(\\sum y_i ^2 - n \\bar{y}^2\\right)}}\n\\]\nYou may have seen a different formula to calculate this quantity which often looks a bit different\n\\[\n\\rho = Corr(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{var(X)var(Y)}}\n\\]\nThe sample correlation coefficient, \\(r\\), is an estimator of the population correlation coefficient, \\(\\rho\\), in the same way as \\(\\bar{X}\\) is an estimator of \\(\\mu\\) or \\(S^2\\) is an estimator of \\(\\sigma^2\\) . Now the question is what does this \\(r\\) values mean?\n\n\n\n\n\n\n\nValue\nMeaning\n\n\n\n\n\\(r=1\\)\nThe two variables move together in the same direction in a perfect linear relationship.\n\n\n\\(0 &lt; r &lt; 1\\)\nThe two variables tend to move together in the same direction but there is NOT a direct relationship.\n\n\n\\(r= 0\\)\nThe two variables can move in either direction and show no linear relationship.\n\n\n\\(-1 &lt; r &lt; 0\\)\nThe two variables tend to move together in opposite directions but there is not a direct relationship.\n\n\n\\(r =-1\\)\nThe two variables move together in opposite directions in a perfect linear relationship.\n\n\n\nLet’s calculate the correlation of our stock data.\n\n\nCode\nimport math\nx = df.StockPrice.values\ny = df.Portfolio.values\n\nn = len(x)\n\nx_sum, y_sum =0,0\ns_xx, s_yy, s_xy = 0,0,0\nfor i in range(n):\n    x_sum += x[i]\n    s_xx += x[i]**2\n    y_sum += y[i]\n    s_yy += y[i]**2\n    s_xy += x[i]*y[i]    \n\ns_xx = s_xx - (x_sum)**2/n\ns_yy = s_yy - (y_sum)**2/n\ns_xy = s_xy - (x_sum * y_sum)/n\n\nr = s_xy/math.sqrt(s_xx * s_yy)\n\n# Print with formatted labels\nprint(f\"Sum x: {x_sum:.2f}\")\nprint(f\"Sum y: {y_sum:.2f}\")\nprint(f\"Sₓₓ: {s_xx:.2f}\")\nprint(f\"Sᵧᵧ: {s_yy:.2f}\")\nprint(f\"Sₓᵧ: {s_xy:.2f}\")\nprint(' ')\nprint(f\"r : {r:.2f}\")\n\n\nSum x: 35.66\nSum y: 32.47\nSₓₓ: 8.53\nSᵧᵧ: 6.97\nSₓᵧ: 7.13\n \nr : 0.92"
  },
  {
    "objectID": "posts/corrandreg/index.html#bivariate-analysis",
    "href": "posts/corrandreg/index.html#bivariate-analysis",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Bivariate Analysis",
    "text": "Bivariate Analysis\nThe joint probability density function for \\(X\\) and \\(Y\\) in the bivariate normal distribution is given by:\n\\[\nf_{X,Y}(x, y) = \\frac{1}{2\\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}}\n\\exp\\left( -\\frac{1}{2(1-\\rho^2)} \\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - 2\\rho\\frac{(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X \\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right] \\right)\n\\]\n\nWhen \\(|\\rho| = 1\\), the denominator \\(\\sqrt{1-\\rho^2}\\) in the PDF becomes zero, which might appear problematic. However, what happens in this case is that the joint distribution degenerates into a one-dimensional structure (a line) rather than being a two-dimensional probability density.\n\nTo see why, consider the quadratic term inside the exponential:\n\\[\nQ = \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - 2\\rho \\frac{(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X \\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2}\n\\]\nWhen \\(|\\rho| = 1\\), this quadratic expression simplifies, as shown next.\nStart with the simplified \\(Q\\) when \\(|\\rho| = 1\\):\n\\[\\begin{align*}\nQ &= \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)^2 - 2\\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\cdot \\frac{y-\\mu_Y}{\\sigma_Y} \\right) + \\left( \\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2\\\\\n&=\\left( \\frac{x-\\mu_X}{\\sigma_X} - \\rho \\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2\n\\end{align*}\\]\nThis is a perfect square because the “cross term” cancels out all independent variability of \\(X\\) and \\(Y\\) when \\(|\\rho| = 1\\).\nFor the quadratic term \\(Q\\) to have any non-zero probability density (since it appears in the exponent of the PDF), it must be equal to zero: \\[\n\\frac{x-\\mu_X}{\\sigma_X} - \\rho \\frac{y-\\mu_Y}{\\sigma_Y} = 0\n\\]\nRearranging this equation: \\[\n\\frac{y-\\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X}\n\\]\nMultiply through by \\(\\sigma_Y\\): \\[\ny - \\mu_Y = \\rho \\frac{\\sigma_Y}{\\sigma_X} (x - \\mu_X)\n\\]\nThus:\n\\[\\begin{align*}\n\\mathbb{E}(Y| X=x)&= \\mu_Y + \\rho\\frac{\\sigma_Y}{\\sigma_X}(x-\\mu_X)\\\\\n& = \\mu_Y + \\rho \\frac{\\sigma_Y}{\\sigma_X} (x - \\mu_X)\n\\end{align*}\\]\nThis is the equation of a straight line in the \\((X, Y)\\)-plane. The slope of the line is \\(\\rho \\frac{\\sigma_Y}{\\sigma_X}\\), and the line passes through the point \\((\\mu_X, \\mu_Y)\\). When \\(|\\rho| = 1\\), all the joint probability mass collapses onto this line, meaning \\(X\\) and \\(Y\\) are perfectly linearly dependent.\n\n\nCode\nimport numpy as np\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define the bivariate normal PDF\ndef bivariate_normal_pdf(x, y, mu_x, mu_y, sigma_x, sigma_y, rho):\n    z = (\n        ((x - mu_x) ** 2) / sigma_x**2\n        - 2 * rho * (x - mu_x) * (y - mu_y) / (sigma_x * sigma_y)\n        + ((y - mu_y) ** 2) / sigma_y**2\n    )\n    denominator = 2 * np.pi * sigma_x * sigma_y * np.sqrt(1 - rho**2)\n    return np.exp(-z / (2 * (1 - rho**2))) / denominator\n\n# Parameters\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\n\n# Function to plot the bivariate normal distribution and a line for rho = 1 or -1\ndef plot_bivariate_and_line_side_by_side(rho1, rho2):\n    fig = plt.figure(figsize=(8, 4))\n\n    # Plot for the first rho\n    ax1 = fig.add_subplot(121, projection='3d')\n    if abs(rho1) == 1:\n        # Degenerate case: Straight line\n        line_x = np.linspace(-3, 3, 100)\n        line_y = line_x  # Since rho = 1 implies y = x (perfect correlation)\n        ax1.plot(line_x, line_y, np.zeros_like(line_x), label=f'Degenerate Line (ρ = {rho1})', color='red')\n    else:\n        # General bivariate normal distribution\n        Z = bivariate_normal_pdf(X, Y, 0, 0, 1, 1, rho1)\n        ax1.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.8)\n\n    ax1.set_title(f'Bivariate Normal (ρ = {rho1:.2f})')\n    ax1.set_xlabel('X')\n    ax1.set_ylabel('Y')\n    ax1.set_zlabel('PDF')\n\n    # Plot for the second rho\n    ax2 = fig.add_subplot(122, projection='3d')\n    if abs(rho2) == 1:\n        # Degenerate case: Straight line\n        line_x = np.linspace(-3, 3, 100)\n        line_y = line_x  # Since rho = 1 implies y = x (perfect correlation)\n        ax2.plot(line_x, line_y, np.zeros_like(line_x), label=f'Degenerate Line (ρ = {rho2})', color='red')\n    else:\n        # General bivariate normal distribution\n        Z = bivariate_normal_pdf(X, Y, 0, 0, 1, 1, rho2)\n        ax2.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none', alpha=0.8)\n\n    ax2.set_title(f'Bivariate Normal (ρ = {rho2:.2f})')\n    ax2.set_xlabel('X')\n    ax2.set_ylabel('Y')\n    ax2.set_zlabel('PDF')\n\n    plt.tight_layout()\n    plt.show()\n\n# Plot examples side by side\nplot_bivariate_and_line_side_by_side(0.5, 1)  # Example with rho = 0.5 and rho = 1\n\n\n\n\n\n\n\n\n\n\n\\(t-\\)Statistic\nUnder the null hypothesis, where \\(H_0: \\rho =0, \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\\) has a \\(t-\\) distribution with \\(\\nu=n-2\\) degree of freedom.\n\n\nFisher’s Transformation of \\(r\\)\nIf \\(W = \\frac{1}{2}\\ln{\\frac{1+r}{1-r}}=\\tanh^{-1}r\\), then \\(W\\) has approximately a normal distribution with mean \\(\\frac{1}{2}\\ln{\\frac{1+\\rho}{1-\\rho}}\\) and standard deviation \\(\\frac{1}{\\sqrt{n-3}}\\).\nFor our stock data:\nNull Hypothesis \\(H_0\\): There is no association between stock prices and the portfolio values, i.e., \\(\\rho =0\\)\nAlternative Hypothesis \\(H_1\\): There is some association between the stock price and portfolio values, i.e., \\(\\rho &gt; 0\\)\nIf \\(H_0\\) is true, then the test statistic \\(\\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}=\\frac{0.92\\sqrt{8}}{\\sqrt{1-0.92^2}}=6.64\\) has a \\(t_8\\) distribution. The observed value \\(6.64\\) is much greater than the critical value of \\(t_8\\) at \\(0.5\\%\\) level which is \\(3.36\\).\nSo, we reject the null hypothesis \\(H_0\\) at the \\(0.5\\%\\) level and conclude that there is a very strong evidence that \\(\\rho&gt;0\\).\nAlternatively, if we want to use the Fisher’s test:\nIf \\(H_0\\) is true, then the test statistic \\(Z_r=\\tanh^{-1}r=\\tanh^{-1}(0.92)\\) has a \\(N\\left(0,\\frac{1}{7}\\right)\\) distribution.\nThe observed value of this statistic is \\(\\frac{1}{2}\\log{\\frac{1+0.92}{1-0.92}}=1.589\\), which corresponds to a value of \\(\\frac{1.589}{\\sqrt{\\frac{1}{7}}}=4.204\\) on the \\(N(0,1)\\) distribution. This is much greater than \\(3.090\\), the upper \\(0.1\\%\\) point of the standard normal distribution.\nSo, we reject \\(H_0\\) at the \\(0.1\\%\\) level and conclude that there is very strong evidence that \\(\\rho &gt; 0\\) ie that there is a positive linear correlation between the stock price and portfolio value."
  },
  {
    "objectID": "posts/corrandreg/index.html#regression-analysis",
    "href": "posts/corrandreg/index.html#regression-analysis",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "Regression Analysis",
    "text": "Regression Analysis\nGiven a set of points \\((x_i,y_i)_{i=0}^{n}\\) for a simple linear regression of the form\n\\[\nY_i = \\alpha +\\beta x_i + \\epsilon_i; \\hspace{4mm} i=1,2,\\cdots,n\n\\]\nwith \\(\\mathbb{\\epsilon_i}=0\\) and \\(var[\\epsilon_i]=\\sigma^2\\).\n\nModel Fitting\nWe can estimate the parameters from the method of least squares but that’s not the goal in this case. Fitting the model involves finding \\(\\alpha\\) and \\(\\beta\\) and the estimating the variance \\(\\sigma^2\\).\n\\[\n\\hat{y} = \\hat{\\alpha}+\\hat{\\beta}x\n\\]\nwhere, \\(\\hat{\\beta}= \\frac{S_{xy}}{S_{xx}}\\) and \\(\\hat{\\alpha} = \\bar{y}-\\hat{\\beta}\\bar{x}\\)\n\\(\\hat{\\beta}\\) is the observed value of a statistic \\(\\hat{B}\\) whose sampling distribution has the following properties\n\\[\n\\mathbb{E}[\\hat{B}]=\\beta, \\hspace{4mm} var[\\hat{B}]=\\frac{\\sigma^2}{S_{xx}}\n\\]\nAnd the estimate of the error variance\n\\[\\begin{align*}\n\\hat{\\sigma}^2 & =\\frac{1}{n-2}\\sum (y_i -\\hat{y_i})^2\\\\\n& = \\frac{1}{n-2} \\left(S_{yy}-\\frac{S_{xy}^2}{S_{xx}}\\right)\n\\end{align*}\\]\n\n\nGoodness of fit\nTo better understand the goodness of fit of the model for the data at hand, we can study the total variation in the responses, as given by\n\\[\nS_{yy} = \\sum (y_i-\\bar{y})^2\n\\]\nLet’s see how:\n\\[\\begin{align*}\ny_i - \\bar{y} &= (y_i - \\hat{y_i}) + (\\hat{y_i}-\\bar{y}) \\\\\n\\implies (y_i - \\bar{y})^2 & = \\left((y_i - \\hat{y_i}) + (\\hat{y_i}-\\bar{y})\\right)^2\\\\\n& = (y_i - \\hat{y_i})^2 + 2(y_i - \\hat{y_i})(\\hat{y_i}-\\bar{y})+(\\hat{y_i}-\\bar{y})^2\\\\\n& = (y_i - \\hat{y_i})^2 + 2 [y_i -(\\hat{\\alpha}+\\hat{\\beta}x_i)][\\hat{\\alpha}+\\hat{\\beta}x_i-(\\hat{\\alpha}+\\hat{\\beta}\\bar{x})]+(\\hat{y_i}-\\bar{y})^2\\\\\n& = (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\left(y_i -\\hat{\\alpha}-\\hat{\\beta}x_i\\right)(x_i-\\bar{x})+(\\hat{y_i}-\\bar{y})^2\\\\\n\\implies \\sum (y_i - \\bar{y})^2 & =\\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\sum\\left(y_i -\\hat{\\alpha}-\\hat{\\beta}x_i\\right)(x_i-\\bar{x})+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& =\\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\left[\\sum x_iy_i-\\bar{x}\\sum y_i -\\hat{\\alpha}\\sum x_i + n\\hat{\\alpha}\\bar{x}-\\hat{\\beta}\\sum x_i^2\\right.\\\\\n&\\left.\\hspace{4mm}+\\hat{\\beta}\\bar{x}\\sum x_i\\right]+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& =\\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}\\left(\\sum x_iy_i-n\\bar{x}\\bar{y}\\right)-2\\hat{\\beta}^2\\left(\\sum x_i^2 - n\\bar{x}^2\\right)+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& = \\sum (y_i - \\hat{y_i})^2 + 2 \\hat{\\beta}S_{xy}-2\\hat{\\beta}^2S_{xx}+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n& = \\sum (y_i - \\hat{y_i})^2 + 2 \\frac{S_{xy}}{S_{xx}}S_{xy}-2\\left(\\frac{S_{xy}}{S_{xx}}\\right)^2S_{xx}+\\sum(\\hat{y_i}-\\bar{y})^2\\\\\n\\implies \\sum (y_i - \\bar{y})^2 & =\\sum (y_i - \\hat{y_i})^2 +\\sum(\\hat{y_i}-\\bar{y})^2\\\\\nSS_{TOT} & = SS_{RES}+ SS_{REG}\n\\end{align*}\\]\nIn the case that the data are “close” to a line ( \\(|r|\\) high- a strong linear relationship) the model fits well, the fitted responses (the values on the fitted line) are close to the observed responses, and so \\(SS_{REG}\\) is relatively high with \\(SS_{RES}\\) relatively low.\nIn the case that the data are not “close” to a line ( \\(|r|\\) low - a weak linear relationship) the model does not fit so well, the fitted responses are not so close to the observed responses, and so \\(SS_{REG}\\) is relatively low and \\(SS_{RES}\\) relatively high.\nThe proportion of the total variability of the responses “explained” by a model is called the coefficient of determination, denoted \\(R^2\\) .\n\\[\nR^2 = \\frac{SS_{REG}}{SS_{TOT}} =\\frac{S_{xy}^2}{S_{xx}S_{yy}}\n\\]\nwhich takes value between 0 to 1, inclusive. The higher \\(R^2\\), the better fitting.\nFor our data, we have:\n\\[\\begin{align*}\nn & = 10, \\hspace{4mm} \\sum x = 35.66, \\hspace{4mm} \\sum y = 32.47\\\\\nS_{xx} &= 8.53 \\hspace{4mm} S_{yy}=6.97, \\hspace{4mm} S_{xy}=7.13\\\\\n\\implies \\hat{\\beta} &=\\frac{S_{xy}}{S_{xx}}= \\frac{7.13}{8.53} = 0.836\\\\\n\\hat{\\alpha} &= \\frac{\\sum y}{n} - \\hat{\\beta} \\frac{\\sum x}{n} = \\bar{y}-\\hat{\\beta}\\bar{x}\\\\\n& = 3.247 - 0.836 \\times 3.566 = 0.266\n\\end{align*}\\]\nTherefore, the fitted line would be \\(\\hat{y}=0.266 + 0.836x\\). Now we see the other metrics\n\\[\\begin{align*}\nSS_{TOT} &= 6.97 \\\\\nSS_{REG} & = \\frac{S_{yy}^2}{S_{xx}} = \\frac{6.97^2}{8.53}=5.695\\\\\nSS_{RES} & = 6.97 - 5.695 = 1.275 \\\\\n\\implies \\hat{\\sigma}^2 & = \\frac{1.275}{8}=0.1594\\\\\nR^2 & = \\frac{5.695}{6.97}=0.817\n\\end{align*}\\]\n\n\nCode\n# Parameters for the line\nalpha = 0.266  \nbeta = 0.836   \n\n# Line values\nline_x = np.linspace(min(df.StockPrice), max(df.StockPrice), 100)  \nline_y = alpha + beta * line_x             \n\n# Plot\nplt.scatter(df.StockPrice, df.Portfolio, color='blue', label='Data Points')\nplt.plot(line_x, line_y, color='red', label=f'Line: y = {alpha} + {beta}x')\n\n# Labels and title\nplt.xlabel('Stock Price')\nplt.ylabel('Portfolio')\nplt.title('Scatter Plot with Regression Line')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nInference on \\(\\beta\\)\nWe can rewrite \\(\\hat{\\beta}= \\frac{S_{xy}}{S_{xx}}\\), as\n\\[\\begin{align*}\n\\hat{\\beta}&= \\frac{S_{xy}}{S_{xx}}=\\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})y_i-\\bar{y}\\sum (x_i-\\bar{x})}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x}y_i)-\\bar{y}\\left(\\sum x_i -n\\bar{x}\\right)}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})y_i}{S_{xx}}\n\\end{align*}\\]\nNow we recall that \\(\\hat{B}\\) is the random variable that has \\(\\hat{\\beta}\\) as its realization. Therefore, \\(\\hat{B}=\\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}}\\). We also recall that \\(\\mathbb{E}(Y_i)=\\alpha +\\beta x\\). Putting these together we obtain,\n\\[\\begin{align*}\n\\mathbb{E}[\\hat{B}] &= \\mathbb{E}\\left[\\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}}\\right] = \\frac{\\sum (x_i -\\bar{x})\\mathbb{E}[Y_i]}{S_{xx}}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})(\\alpha + \\beta x_i)}{S_{xx}}\\\\\n& = \\frac{\\alpha \\sum (x_i-\\bar{x})+\\beta \\sum x_i (x_i-\\bar{x})}{S_{xx}}\\\\\n& = \\frac{\\alpha \\left(\\sum x_i -n\\bar{x}\\right)+\\beta \\left(\\sum x_i^2-\\bar{x}\\sum x_i\\right)}{S_{xx}} \\\\\n& = \\frac{\\alpha (n\\bar{x}-n\\bar{x})+\\beta\\left(\\sum x_i^2-n\\bar{x}^2\\right)}{S_{xx}}\\\\\n& = \\frac{0+\\beta S_{xx}}{S_{xx}} = \\beta\n\\end{align*}\\]\nNow the fact that \\(Y_i'\\)s are uncorrelated. Therefore, \\(var\\left(\\sum (Y_i)\\right)=\\sum var(Y_i)\\) and we have \\(var(Y_i)=\\sigma^2\\). Therefore,\n\\[\\begin{align*}\nvar[\\hat{B}]& = var\\left[\\frac{\\sum (x_i-\\bar{x})Y_i}{S_{xx}}\\right]= \\frac{\\sum (x_i-\\bar{x})^2var[Y_i]}{S_{xx}^2}\\\\\n& = \\frac{\\sum (x_i-\\bar{x})^2\\sigma^2}{S_{xx}^2} = \\frac{\\sigma^2}{S_{xx}^2}\\sum (x_i-\\bar{x})^2 = \\frac{\\sigma^2}{S_{xx}^2}S_{xx}\\\\\n& = \\frac{\\sigma^2}{S_{xx}}\n\\end{align*}\\]\nSince \\(\\mathbb{E}(\\hat{\\beta})=\\beta\\) and \\(var(\\hat{\\beta})=\\frac{\\sigma^2}{S_{xx}}\\) so\n\\[\nM = \\frac{\\hat{\\beta}-\\beta}{\\sqrt{\\frac{\\sigma^2}{S_{xx}}}}\\sim N(0,1)\n\\]\nand the observed variance \\(\\hat{\\sigma}^2\\) has the property\n\\[\nN = \\frac{(n-2)\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi_{n-2}^2\n\\]\nSince \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) are independent, it follows that\n\\[\n\\frac{M}{\\sqrt{\\frac{N}{n-2}}} \\sim t_{n-2}\n\\]\nIn other words: \\[\n\\frac{\\hat{\\beta}-\\beta}{se(\\hat{\\beta})} = \\frac{\\hat{\\beta}-\\beta}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\sim t_{n-2}\n\\]\nNow the big question is what’s the use of this mathematical jargon that we have learned so far? Let’s use our regression problem on stock data to explain.\n\\(H_0: \\beta =0\\), there is no linear relationship\nvs\n\\(H_1: \\beta&gt; 0\\), there is a linear relationship\nBased on our data we have \\(\\hat{\\beta} = 0.836\\) and \\(\\hat{\\sigma}^2=0.1594\\), and \\(S_{xx}=8.53\\). Therefore, under \\(H_0\\), the test statistic\n\\[\n\\frac{\\hat{\\beta}-0}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\text{ has a } t_{10-2} \\text{ or } t_8 \\text{ distribution}\n\\]\nBut the observed value of this statistic \\[\n\\frac{0.836-0}{\\sqrt{0.1594/8.53}}=6.1156\n\\]\nwhich is way higher than the critical value at \\(5\\%\\) significance level.\n\n\nCode\nfrom scipy.stats import t\n\n# Parameters\ndf = 8  # Degrees of freedom\nalpha = 0.05  # Upper tail probability\nt_critical = t.ppf(1 - alpha, df)  # Critical t-value at the 95th percentile\n\n# Generate x values for the t-distribution\nx = np.linspace(-4, 4, 500)\ny = t.pdf(x, df)\n\n# Plot the t-distribution\nplt.plot(x, y, label=f't_{df} Distribution', color='blue')\nplt.fill_between(x, y, where=(x &gt;= t_critical), color='red', alpha=0.5, label=f'Upper {alpha*100}% Area')\n\n# Annotate the critical t-value on the x-axis\nplt.axvline(t_critical, ymin=0.02, ymax=0.30,color='red', linestyle='--', label=f'Critical t-value = {t_critical:.2f}')\nplt.text(t_critical, -0.02, f'{t_critical:.2f}', color='red', ha='center', va='top')\n\n# Add a horizontal line at y = 0\nplt.axhline(0, color='black', linestyle='-', linewidth=0.8)\n\n# Labels, title, and legend\nplt.title(f\"t-Distribution with {df} Degrees of Freedom\")\nplt.xlabel(\"t\")\nplt.ylabel(\"Density\")\nplt.legend()\n\n# Adjust plot limits\n\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nSo, we reject the null hypothesis \\(H_0\\) at the \\(5\\%\\) level and conclude that there is a very strong evidence that \\(\\beta&gt;0\\), i.e., the portfolio value is increasing over stock price.\nAlternatively, let’s put our analysis in a different approach. We claim that\n\\(H_0: \\beta=1\\), there is a linear relationship\nvs\n\\(H_1: \\beta \\ne 1\\)\nIn this case,\n\\[\nse(\\hat{\\beta}) = \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} = \\sqrt{\\frac{0.1594}{8.53}} =0.1367\n\\]\nTherefore, the \\(95\\%\\) confidence interval for \\(\\beta\\) is\n\\[\n\\hat{\\beta} \\pm \\left\\{t_{0.025,8}\\times se(\\hat{\\beta})\\right\\}=0.836 \\pm 2.306\\times 0.1367 = (0.5207,1.1512)\n\\]\nThe \\(95\\%\\) two-sided confidence interval contains the value \\(1\\), so the two-sided test conducted at \\(5\\%\\) level results in \\(H_0\\) being accepted.\n\n\nMean Response and Individual Response\n\nMean Response\n\nIf \\(\\mu_0\\) is the expected (mean) response for a value \\(x_0\\) of the predictor variable, that is \\(\\mu_0 = \\mathbb{E}[Y|x_0]=\\alpha +\\beta x_0\\), then \\(\\mu_0\\) is an unbiased estimator given by\n\n\\[\n\\hat{\\mu}_0 = \\hat{\\alpha}+\\hat{\\beta} x_0\n\\]\nand the variance of the estimator is given by\n\\[\nvar(\\hat{\\mu}_0) = \\left(\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2\n\\]\nTherefore,\n\\[\n\\frac{\\hat{\\mu}_0-\\mu_0}{se[\\hat{\\mu}_0] }= \\frac{\\hat{\\mu}_0-\\mu_0}{\\sqrt{\\left(\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2}}\\sim t_{n-2}\n\\]\n\n\nIndividual Response\nThe actual estimate of an individual response \\[\n\\hat{y}_0 = \\hat{\\alpha} +\\hat{\\beta}x_0\n\\]\n\nHowever, the uncertainty associated with this estimator, as indicated by its variance, is higher compared to the mean estimator because it relies on the value of an individual response \\(y_0\\) rather than the more stable mean. To account for the additional variability of an individual response relative to the mean, an extra term, \\(\\sigma^2\\), must be included in the variance expression for the estimator of a mean response.\n\n\\[\nvar[\\hat{y}_0] = \\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2\n\\]\nThus,\n\\[\n\\frac{\\hat{y}-y_0}{se[\\hat{y}_0]}=\\frac{\\hat{y}-y_0}{\\sqrt{\\left(1+\\frac{1}{n}+\\frac{(x_0-\\bar{x})^2}{S_{xx}}\\right)\\sigma^2}}\\sim t_{n-2}\n\\]\n\nLet’s put this two idea through our example. If we want to find a \\(95\\%\\) confidence interval or the expected portfolio value on stock price of say, 360. In that case,\n\n\\[\n\\text{Estimate of the expected portfolio value} = 0.266+0.836\\times 3.6 = 3.276\n\\]\nand\n\\[\n\\text{se}[\\text{Estimate}] = \\sqrt{\\left(\\frac{1}{10}+\\frac{(3.6-3.566)^2}{8.53}\\right) 0.1594}=0.1263\n\\]\nSo, the \\(95\\%\\) CI\n\\[\n3.276\\pm (t_{0.025,8}\\times \\text{se}[\\text{Estimate}]) = 3.276 \\pm 2.306\\times 0.1263 = (2.985,3.567)\n\\]\nThat is for a stock price of \\(\\$360\\), the expected portfolio value would be in the range of \\((\\$298.50,\\$356.70)\\)\nSimilarly, the \\(95\\%\\) CI for the predicted actual portfolio value\n\\[\\begin{align*}\n3.276\\pm (t_{0.025,8}\\times \\text{se}[\\text{Estimate}]) &= 3.276 \\pm 2.306\\sqrt{\\left(1+\\frac{1}{10}+\\frac{(3.6-3.566)^2}{8.53}\\right) 0.1594}\\\\\n& = (2.3103,4.2417)\n\\end{align*}\\]\nor \\((\\$231.03,\\$424.17)\\)\n\n\n\nModel Accuracy\nThe residual from the fit at \\(x_i\\) is the estimated error which is defined by \\[\n\\hat{\\epsilon}_i = y_i - \\hat{y}_i\n\\]\n\nScatter plots of residuals versus the explanatory variable (or the fitted response values) are particularly insightful. A lack of random scatter in the residuals, such as the presence of a discernible pattern, indicates potential shortcomings in the model.\n\n\n\nCode\ndf = pd.DataFrame({\n    'Stock': ['Apple', 'Citi', 'MS', 'WF', 'GS', 'Google', 'Amazon', 'Tesla', 'Toyota', 'SPY'],\n    'StockPrice': [2.11, 2.42, 2.52, 3.21, 3.62, 3.86, 4.13, 4.27, 4.51, 5.01], \n    'Portfolio': [2.12, 2.16, 2.51, 2.65, 3.62, 3.15, 4.32, 3.31, 4.18, 4.45]\n})\nx = df.StockPrice.values\ny = df.Portfolio.values \n\ny_hat = [0.266+0.836*i for i in x]\nplt.scatter(x, y-y_hat)\nplt.axhline(0)\nplt.ylabel('Residuals')\nplt.xlabel('Stock Price')\nplt.title('Scatter plot of the residuals from the fitted line')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn this plot, we can see that the residuals tend to increase as \\(x\\) increases, indicates that the error variance is not bounded, but increasing with \\(x\\). So, the model is not the best one. A transformation of the responses may stabilize the error variance.   In certain case, for some growth models, the appropriate model is that the expected response is related to the exploratory variable through an exponential relationship, i.e.,\n\n\\[\\begin{align*}\n\\mathbb{E}[Y_i|X=x_i] &= \\alpha e^{\\beta x_i}\\\\\n\\implies z_i = \\log y_i & = \\eta + \\beta x_i + \\epsilon_i; \\hspace{4mm}\\text{where }\\eta=\\log \\alpha\n\\end{align*}\\]\n\n\nCode\nx = df.StockPrice.values\ny = np.log(df.Portfolio.values)\n\nn = len(x)\n\nx_sum, y_sum =0,0\ns_xx, s_yy, s_xy = 0,0,0\nfor i in range(n):\n    x_sum += x[i]\n    s_xx += x[i]**2\n    y_sum += y[i]\n    s_yy += y[i]**2\n    s_xy += x[i]*y[i]    \n\ns_xx = s_xx - (x_sum)**2/n\ns_yy = s_yy - (y_sum)**2/n\ns_xy = s_xy - (x_sum * y_sum)/n\n\nr = s_xy/math.sqrt(s_xx * s_yy)\n\n# Print with formatted labels\nprint(f\"Sum x: {x_sum:.2f}\")\nprint(f\"Sum y: {y_sum:.2f}\")\nprint(f\"Sₓₓ: {s_xx:.2f}\")\nprint(f\"Sᵧᵧ: {s_yy:.2f}\")\nprint(f\"Sₓᵧ: {s_xy:.2f}\")\nprint(' ')\nprint(f\"r : {r:.2f}\")\n\n\nSum x: 35.66\nSum y: 11.43\nSₓₓ: 8.53\nSᵧᵧ: 0.70\nSₓᵧ: 2.29\n \nr : 0.94\n\n\nNow we have:\n\\[\\begin{align*}\nn & = 10, \\hspace{4mm} \\sum x = 35.66, \\hspace{4mm} \\sum y = 11.43\\\\\nS_{xx} &= 8.53 \\hspace{4mm} S_{yy}=0.70, \\hspace{4mm} S_{xy}=2.29\\\\\n\\implies \\hat{\\beta} &=\\frac{S_{xy}}{S_{xx}}= \\frac{2.29}{8.53} = 0.268\\\\\n\\hat{\\alpha} &= \\frac{\\sum y}{n} - \\hat{\\beta} \\frac{\\sum x}{n} = \\bar{y}-\\hat{\\beta}\\bar{x}\\\\\n& = 1.143 - 0.268 \\times 3.566 = 0.1873\n\\end{align*}\\]\n\n\nCode\nimport numpy as np\nz_hat = [np.log(0.1873)+0.268*i for i in x]\nz = np.log(y)\nplt.scatter(x, z-z_hat)\nplt.axhline(np.mean(z-z_hat))\nplt.ylabel('Residuals')\nplt.xlabel('Stock Price')\nplt.title('Scatter plot of the residuals from the fitted line')\nplt.show()\n\n\n\n\n\n\n\n\n\nNow the residuals look good, that is no special pattern or increasing the error variance.\nThanks for reading."
  },
  {
    "objectID": "posts/corrandreg/index.html#references",
    "href": "posts/corrandreg/index.html#references",
    "title": "Correlation, Bivariate, and Regression Analysis",
    "section": "References",
    "text": "References\n\nMontgomery, D. C., & Runger, G. C. (2014). Applied Statistics and Probability for Engineers. Wiley.\n\nCasella, G., & Berger, R. L. (2002). Statistical Inference. Duxbury.\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences. Routledge.\n\nSeber, G. A. F., & Lee, A. J. (2003). Linear Regression Analysis. Wiley.\nNeter, J., Kutner, M. H., Nachtsheim, C. J., & Wasserman, W. (1996). Applied Linear Statistical Models. Irwin.\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\n\nWeisberg, S. (2005). Applied Linear Regression. Wiley.\n\nBivariate Normal Distribution Explanation:\n\nRice, J. A. (2006). Mathematical Statistics and Data Analysis. Thomson Brooks/Cole.\n\nA detailed exploration of the bivariate normal distribution and its properties.\n\nFisher’s Transformation of Correlation Coefficients:\n\nFisher, R. A. (1921). On the probable error of a coefficient of correlation. Metron, 1, 3-32.\n\nThe foundational paper describing Fisher’s transformation and its use in hypothesis testing.\n\n\nShare on\n\n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n\nYou may also like"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basic Statistics and Probability",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPricing Derivatives Using Black-Scholes-Merton Model\n\n\n\nProgramming\n\nFinance\n\nFinancial Mathematics\n\nPricing\n\nHedging\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nSome Key Statistical Concepts for Interview Prep\n\n\n\nData Science\n\nMachine Learning\n\nArtificial Intelligence\n\nStatistics\n\n\n\n\n\n\n\n\n\nMay 9, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Inference in Machine Learning: Part 1\n\n\n\nMachine Learning\n\nData Science\n\nBayesian Inference\n\nBayesian Statistics\n\nStatistics\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nMonte-Carlo Methods: PRNGs\n\n\n\nData Science\n\nComputational Mathematics\n\nNumber Theory\n\n\n\n\n\n\n\n\n\nAug 11, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nReview probabilities\n\n\n\nProbability\n\nStatistics\n\n\n\n\n\n\n\n\n\nAug 22, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation, Bivariate, and Regression Analysis\n\n\n\nData Science\n\nMachine Learning\n\nArtificial Intelligence\n\nStatistics\n\n\n\n\n\n\n\n\n\nDec 18, 2024\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Law of Large Numbers and Central Limit Theorem\n\n\n\nStatistics\n\nProbability\n\n\n\n\n\n\n\n\n\nMar 15, 2025\n\n\nRafiq Islam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n  \n\n Back to top"
  }
]